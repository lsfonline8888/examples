[["index.html", "R语言学习案例 第 1 章 前言 1.1 参考文献", " R语言学习案例 TJUFE, Sifan Liu 2022-08-05 第 1 章 前言 1.1 参考文献 王汉生,成慧敏.商务数据分析与应用——基于R（第2版）[M].北京：中国人民大学出版社，2020. "],["线性回归以移动通信网络的客户价值分析为例.html", "第 2 章 线性回归——以移动通信网络的客户价值分析为例 2.1 背景介绍 2.2 案例介绍 2.3 指标设计 2.4 描述性统计分析 2.5 线性回归理论 2.6 R代码实现", " 第 2 章 线性回归——以移动通信网络的客户价值分析为例 2.1 背景介绍 企业管理者需要在现有的客户资源中，区分并发现对自己最有价值的客户。传统的营销智慧告诉我们：往往是20%的最有价值客户，贡献了企业80%的利润。 找到最有价值的客户，对于公司高效运营意义重大。 在实际操作中，这绝非易事。首先须面对的挑战就是:什么是客户价值？如何测量？显然，对于不同的行业、不同的企业、不同的时期，答案各不相同。 2.2 案例介绍 运营商的主要业务是向客户提供无线通信服务，并收取相应的费用。三大运营商（移动、联通、电信）互相挖墙脚，增加规模开始变得艰难。 很多客户今天用A公司的服务，明天只要B公司给一点点好处，就改用B公司的，再过几天A公司有新的打折促销计划，又换回A公司。 如何增强客户忠诚度，提高盈利能力，对公司来说十分重要。为此，移动运营商推出了校园网计划。 校园网计划是一个客户忠诚度的培养计划。该运营商关注的客户群体中很重要的一部分是高校在校生。 高校在校生本身是一批非常优质的客户。按照校园网运营规则，如果一名高校在校生希望加入校园网，他首先必须是该运营商的用户。此外，还得由现有校园网用户进行邀请。作为回报，所有的校园网内通话资费都会非常便宜，而且有非常大的数据流量优惠，但与网外朋友通话资费照旧。 所以，为了进一步降低自身的资费水平，现有校园网成员有很大的动力邀请朋友加入校园网。同时，已经加入校园网的成员则发现很难离开，因为大部分朋友及主要的社交网络都在校园网中，个人一旦离开，还想保持跟过去一样的沟通强度，成本昂贵。 为了深度“套牢”在校生客户，运营商有重要付出，即降低资费。此外，为了迅速扩张，鼓励大家推荐新客户，运营商对推荐者有一定的奖励。例如，成功推荐一名用户，推荐者将获得10元话费+1GB数据流量。 通过一段时间的运营发现: (1)好消息是离网率确实下降不少，总利润有所上升； (2)坏消息是总利润上升低于预期。 为什么?业务员反馈回来一个重要的信息:由于缺乏可靠的手段识别被邀请的用户是否真的是高校在校生，很多低端客户被加进来，而他们并不是高校在校生。这部分客户的总消费量（如通话总量）并没有因为入网而有所上升；相反，由于资费的下降，他们给公司贡献的收入大幅下降。当然，这是负面案例，并不是全部的情况。也有正面案例，有的客户自己入网后，能够进一步吸引一大批优质客户入网。相比入网前，他们的沟通交流更加密切。因此，尽管单位时长的资费水平下降很多，但是他们对企业的总利润贡献上升不少。 因此，需要详细研究：什么样的推荐者能够带来高（或低）价值客户？这样，才能把有限的奖励资源有针对性地投入到那些能够为企业带来高价值客户的推荐者身上。 2.3 指标设计 哪个指标能够刻画推荐者价值，就是本章的因变量。 首先，推荐者本身也是校园网的普通消费者。因此，毋庸置疑，他对企业的直接利润贡献是其价值的一个重要组成。为方便起见，简称这部分价值为该客户的直接价值。但是，本案例更关注推荐者通过推荐其他客户带来的间接价值。 如何量化一个推荐者的间接价值?假如一名推荐者为企业推荐了三名客户，他为企业带来了多少利润?这依赖于这三名客户在被推荐前后的行为变化。如果在被推荐加入校园网之前，他们每个月总共贡献利润100元，加入校园网后变成了80元，那么这就是一个失败的推荐者，他的推荐行为为企业带来的利润相对变化为:(80-100)/100=20%，如果在加入校园网后 这三名客户的利润贡献是120元，那么推荐者对企业的间接利润贡献为(120-100)/100=20%，因此，本章的因变量就是某推荐者所有推荐客户在加入校园网前后的相对利润变化率。 确定了因变量以后，再考虑解释变量，对一个自然人的描述依靠高矮胖瘦等指标，对一个推荐者(消费者)的刻面就得靠具有营销实践意义的消费者特征或者标签，在实际工作中，研究者积累了大量的有用指标(或者标签)，能够比较详细地刻画一个推荐者的方方面面。例如可以考虑消费者的消费行为，主要包括该用户在各项通信及增值业务(如通话、短信、彩铃、上网)上的花费；还可以考虑消费者的通话特征，包括该用户的通话时长、通话频率、通话时间(早上、中午、晚上)；还可以进一步地将通语时长拆分成主叫、被叫、本地、长途、漫游等。总而言之，实际工作中可以考虑的标签很多，它们有助于更好地描述目标客户(推荐者)，而这些标签统统都可以成为解释变量X。就本案例而言，为简单起见，只考虑下面几种标签。 1.通话质量(\\(X_1\\)) 通话总量即总的通话时长，以百分钟计。毫无疑向，这是一个很重要的变量，它直接刻画了用户的活跃程度。在单位时间内(如一个月内)，通话总量高的用户更加活跃。由于校园网提供非常优惠的通话资费，因此对那些通话总量高的用户有很强的吸引力。假设高通话量的用户的好友(被推荐者)也更有可能是高通话量用户(也是“话痨”)，那么具有高通话总量特征的推荐者就更有可能带来优质客户。 2.大网占比(\\(X_2\\)) 所谓大网就是该运营商的无线通信网络。大网占比衡量了在用户的所有通话时长中，有多少发生在该运营商的网内。假设该运营商是联通，那么一个人的通话总量可能发生在联通内部，也可能发生在联通外部(移动、电信)。如果将一个人的通话总量(联通、移动、电信、其他)看作他基于移动通信的社会关系网络的全部，那么所谓大网占比就是该推荐者的移动通信社会关系网络中，有多少被该运营商(如联通)所覆盖。这个占比越高，说明该用户基于移动通信的社交网络越多地发生在该运营商生态体系内。因此，该用户的潜力巨大。 3.小网占比(\\(X_3\\)) 所谓小网就是校园网。相对于大网而言，这是一个更小的网络，是大网的子集。小网占比是大网占比的有力补充。大网占比衡量了一个用户全部的移动通信社交网络有多少在该运营商的网络内，小网占比衡量的则是该用户已经发生在大网(即该运营商网络)内的通话时长中，有多少发生在校园网 (即一个更小、有更优惠资费的网络)内。请注意，要加入校园网的一个前提条件是：该用户已经是该运营商的用户。因此，一个推荐者能推荐发展多少用户是有上限的，即该用户所有的发生在该运营商网络内部的社交关系。如果一名用户小网占比很高，说明该用户可被推荐的社会关系网络(请注意，大网以外的客户是不能加入小网的)中的绝大部分已经加入了校园网，因此，该用户没有更多的被推荐对象，所以推测他能为企业带来的价值也许不高。 2.4 描述性统计分析 描述分析就是对数据最基本、最简单的描述统计。对于连续型数据而言，基本的描述统计量(如样本均值、标准差、最大最小值、中位数等)都可以考虑计算一下。描述分析除了有助于发现数据问题，还能够被最广泛的受众理解并接受。由于不涉及任何高深的统计学推断方法，因此只要有一定数学基础的人都能够读懂描述分析(如样本均值)。但是，描述分析也有缺点，即无法综合考虑众多因素，因此无法给出一个全面科学的统计学推断。 对本例而言，X和Y都是连续型数据。因此，一个自然的想法是，做一个关于X和Y的散点图，希望从中能够看到大概的趋势。但是，经验表明，对于大多数实际问题，数据噪声很大，从散点图上很难看出清晰的规律。因此，推荐一种简单粗糙但更有效地描述统计方法。具体而言，可以根据因变量Y的取值将数据分成不同的组(如高、中、低)。对本例而言，以因变量Y的中位数为阈值，将数据分成两组(高价值组、低价值组)。然后，对每个X变量做箱线图。 2.5 线性回归理论 2.5.1 统计模型 在描述分析的基础上，进一步分析因变量Y和各个解释变量\\((X_1,X_2,X_3)\\)之间的关系。更进一步地，希望知道解释变量是怎样影响因变量的，其影响强度如何。研究解释变量和因变量之间的关系，需要建立模型。在回归分析的框架下，模型就是一个连接解释变量X和因变量Y之间关系的函数表达式。因此，不妨假设： \\[模型A：Y=f(X)\\] 式中，\\(f(^.)\\)是一个事前设定的函数形式，而\\(X=(X_1,X_2,X_3)&#39;\\)是一个三维向量，代表了所有的解释变量。如果要这个模型付诸实施，需要思考什么样的函数形式f(.)最适合。模型A表达的是一种确定性的函数关系。这显然是一个非常不合理的性质。以本案例为例，即使两个推荐者的所有解释变量完全相同(即相同的通话总量\\(X_1\\)，相同的大网占比\\(X_2\\)，相同的小网占比\\(X_3\\))，他们为企业带来的间接价值Y也不可能完全相同。因为本案例所采集的解释变量X，是影响该推荐者间接价值Y的无穷种因素中的3个而已，除此以外，还有很多其他的影响因素(如性别、收人、性格、情绪等)。即使两个推荐者在现有的3个X变量上取值完全相同，但在其他的未被采集的相关因素上的取值却一定有所不同(否则为什么是两个不同的人呢)。所以，极有可能这两个人的推荐者间接价值Y也是不一样的。那么，有可能通过采集更多的解释变量彻底解决这个问题吗？答案是否定的。因为现实世界中的复杂因素无穷多，而且在不停的动态变化中。所以，一个合理的统计学模型不可能彻底解决所有的不确定性。 那么不确定性又应该如何刻画?在数学层面，应该如何改造模型A，使得它可以兼容不确定性?一个最简单的办法是引入噪声项ε，也叫随机扰动项。因此，对模型A稍作修改，就变成了： \\[模型B：Y=f(X,ε)\\] 式中，\\(f(^.,^.)\\)也是一个事先设定的函数。 假设\\(f(^.,^.)\\)是最普通的线性函数，这就有了线性回归模型。具体数学形式如下： \\[模型C:Y=β_0+β_1X_1+β_2X_2+β_3X_3+ε\\] 式中\\(β=(β_0,β_1,β_2,β_3)&#39;\\)称作回归系数。为了处理方便，要求噪声项ε的期望\\(E(ε)=0\\)。 2.5.2 模型理解 首先考虑模型C中的随机干扰项ε。ε代表的是那些没有被解释变量X捕捉的，但是对因变量Y有影响的，说不清道不明的其他因素，因此，ε是完全不可控的因素，没有太大的关注必要，也没有精确把控的可能性。那么关注的重点应该就是这三个X变量和它们对应的同归系数，弄清这三个X变量和Y的相关关系到底怎样。对于每一个具位的X变量，有三个核心问题： (1)该变量同Y有相关关系吗?尤其是在控制(或者剔除)其他两个X变量的影响后。以本案例\\(X_1\\)为例，在控制大网占比\\(X_2\\)和小网占比\\(X_3\\)不变的前提下，通话总量X的变化是否同推荐间接价值Y相关? (2)如果相关，该X变量同Y的相关关系是正的还是负的?以本案例\\(X_1\\)为例，假设通话总量\\(X_1\\)同Y确实相关，那么它们的相关性是正还是负? (3)假设某X变量同Y确实相关，而且相关关系为正，该相关关系的强度到底如何?以本案例X\\(X_1\\)为例，在控制大网占比\\(X_2\\)和小网占比\\(X_3\\)不变的前提下，通话总量\\(X_1\\)的单位变化能产生多少关于推荐者间接价值的预期变化? 这三个问题的答案统统都隐藏在三个所对应的β系数中。 接下来，做一些更加具体的数学讨论。\\(β=(β_0,β_1,β_2,β_3)&#39;\\)是线性回归模型C的回归系数向量，其中包含了两种不同的系数。一种是截距项\\(β_0\\)，另外一种是普通的系数项\\(β_1\\)，\\(β_2\\)和\\(β_3\\)。它们的意义是不一样的。 针对本例，\\(X = 0\\)代表这样一类推荐者，他们的通话总量为0(\\(X_1=0\\)，每月只打0分钟电话)，大网占比为0 (\\(X_2=0\\)，通话总量都为0了，其实大网占比无法定义)，小网占比也为0(\\(X_3=0\\)通话总量都为0了，其实小网占比也无法定义)。显然，这样的推荐者是不可能存在的。因此，对本案例而言，\\(β_0\\)没有什么好的可解读性。对于这种情况有两个办法。第一个办法，忽略它。第二个办法，可以考虑在建立回归模型之前，先将所有的解释变量中心化，即重新定义:\\(X=X-E(X)\\)，这样做的直接后果就是\\(E(X)=0\\)，再结合模C型，不难验证\\(E(Y)=β_0\\)。因此，在这种情况下，截距项反映的是因变量的简单样本均值。 \\(β_1\\)反映的是在其他解释变量(即大网占比\\(X_2\\)和小网占比\\(X_3\\))保持不变的情况下，通话总\\(X_1\\)的单位变化能带来的对因变量Y的变化预期。 在假设ε与X相互独立的前提下， \\[σ_y^2=var(β_1X_1+β_2X_2+β_3X_3)+σ^2\\] 这说明，因变量Y的变异性\\(σ_y^2\\)是由两种不同的原因造成的。第一种是解释变量，即\\(var(β_1X_1+β_2X_2+β_3X_3)\\)。这部分变异性是解释变量X造成的，因此可以通过解释变量X所解释。第二种是噪声项ε造成的，即\\(σ^2\\)。按照上式，噪声项的变异性是不可能超过因变量的变异性的。因此，可以考虑通过比较\\(σ_y^2\\)和\\(σ^2\\)的相对大小来判断随机噪声项在模型C中所起的作用。 2.5.3 估计方法 假设有n个样本，i代表其中一个特定的个体，因此有1≤i≤n。用\\((Y_i，X_i)\\)代表来自第i个样本的数据，其中\\(Y_i\\)是因变量，而\\(X_i =(X_i1,…,X_ip)&#39;\\)是相应的解释变量。对本案例而言，p=3。一般的线性回归模型允许任意多的解释变量，只要其个数p比样本量n小很多。模型C可以重新表述如下: \\[Y_i=β_0+β_1X_{i1}+…+β_pX_{ip}+ε_i\\] 式中，\\(ε_i\\)是同第i个个体相关的随机扰动。 接下来考虑如何估计\\(β=(β_0,β_1,…,β_p)&#39;\\)。随机噪声看不见摸不着，不可能把握。所 以，不可能对它有所估计或者预测，先把它忽略掉。这样得到下面的近似模型：\\[Y_i≈β_0+β_1X_i1+…+β_pX_ip\\] 好的β估计应该极小化\\(Y_i\\)和\\(β_0+β_1X_{i1}+…+β_pX_{ip}\\)之间的某种距离。可以考虑常用的平方距离： \\[(Y_i-β_0-β_1X_i1-…-β_pX_ip)^2\\] 平方距离应该对每个样本都计算，因此最小二乘目标函数为：\\[Q(β)=\\sum_{i=1}^n(Y_i-β_0-β_1X_i1-…-β_pX_ip)^2\\]那么，\\(\\hat{β}=argmin_βQ(β)\\)。 最小二乘估计不是唯一估计，那么，最小一乘目标函数为：\\[Q_1(β)=\\sum_{i=1}^n\\lvert Y_i-β_0-β_1X_i1-…-β_pX_ip\\lvert\\] 通过极小化这个目标函数，可以获得最小一乘估计，但是由于最小一乘估计的目标函数不是一个光滑可导函数，因此不可能有显式解。 对于某些应用人们有强烈的先验知识。因此，对每个个体的重要性有一个事先的权重判断，记为\\(ω_i\\)。将最小二乘目标函数修改：\\[Q_2(β)=\\sum_{i=1}^nω_i(Y_i-β_0-β_1X_i1-…-β_pX_ip)^2\\] 2.5.4 假设检验 第一个假设问题是：是否至少有一个解释变量有用？对应于下面的全局显著性假设检验问题： \\[H_0:\\tilde{β}=0\\] \\[H_1:\\tilde{β}≠0\\] 其中，\\(\\tilde{β}=(β_1,…,β_p)&#39;\\)。β是一个(p+1)维向量，包括截距项，而\\(\\tilde{β}\\)是一个P维向量，不包含截距项。 怎样做全局检验？可以通过考察\\(RSS_0-RSS_1\\)的大小得到有用的判断。为此，构造了F-统计量： \\[F=\\frac{(RSS_0-RSS_1)/P}{RSS_1/(n-p-1)}\\] 在原假设成立的情况下，该F-统计量服从一个自由度为(p，n-p-1)的F-分布。以此为依据，可以计算出相应的P-值。 如果F-检验无法拒绝原假设，说明在现有的数据基础上，找不到足够证据证明所有解释变量至少有一个是重要的。因此分析到此为止。但是，如果F-检验成功推翻\\(H_0\\)，那就要追问：哪些解释变量重要？需要逐个对解释变量\\(X_j\\)作下面的局部检验： \\[H_0:β_j=0\\] \\[H_0:β_j≠0\\] 为此，可以考虑标准的t-统计量： \\[t=\\frac{\\hat{β_j}}{\\hat{SE}(\\hat{β_j})}\\] 式中，\\(\\hat{SE}(\\hat{β_j})\\)代表最小二乘估计\\(\\hat{β_j}\\)的标准差。在原假设成立的情况下，该t-统计量近似服从一个标准的正态分布。但是需要一个前提假设：即残差ε必须严格服从正态分布。 2.5.5 判决系数 \\(RSS_0\\)是在没有任何解释变量帮助的情况下产生的残差平方和，\\(RSS_1\\)代表因变量中无法被X变量解释的部分。因此，\\(RSS_1/RSS_0\\)刻画了在因变量Y的变异性中，随机噪声项所占的比例。剩余的\\((1-RSS_1/RSS_0)\\)则是由解释变量产生。因此，定义判别系数如下：\\[R^2=(1-\\frac{RSS_1}{RSS_0})×100\\%\\] 判决系数的大小直接反映了解释变量X对因变量Y的拟合情况，也就是拟合优度。 虽然判决系数这么有用，但它有一个致命的缺陷，即它永远偏爱更加更加复杂的模型。这是由判决系数的数值计算性质决定的。任给一个模型，永远可以通过增加解释变量的个数获得判决系数的进一步提高。但是，假设人为地生成一些与实际问题毫不相关的干扰X数据只可能降低模型的预测精度，但是它们仍会使判决系数微小上升。因此，需要调整判决系数。如下： \\[R^2_{adj}=(1-\\frac{n-1}{n-p-1}×\\frac{RSS_1}{RSS_0})×100\\%=(1-\\frac{RSS_1}{n-p-1}×\\frac{n-1}{RSS_0})×100\\%\\] 除了调整后的判决系数\\(R^2_{adj}\\)，还有一个解决方案，就是外样本判决系数\\(R^2_{out}\\)。就是要根据一定的比例(如1:1)，将数据一分为二。第一部分数据用于训练模型估计参数。这部分数据称作训练数据，训练数据的任务是产生最小二乘估计\\(\\hat{β}\\)。第二部分数据用于验证\\(\\hat{β}\\)的预测精度，称为验证数据，用\\(S_0=\\{(Y_i,X_i):1≦i≦n\\}\\)表示训练数据集，\\(S_1=\\{(Y_i^*,X_i^*):1≦i≦n\\}\\)表示验证数据集。如果当初切分数据的时候采用的比例是1:1，那么两个数据集合的样本量应该是一样的，即n=m。然后，外样本判决系数的定义如下: \\[R^2_{out}=[1-\\frac{\\sum_{i=1}^m(Y_i^*-\\hat{β_0}-\\hat{β_1}X_{i1}^*-\\hat{β_2}X_{i2}^*…-\\hat{β_p}X_{ip}^*)^2}{\\sum_{i=1}^m(Y_i^*-\\bar Y^*)^2}]×100\\%\\] 但是\\(R^2_{out}\\)也有缺点。第一，要采纳外样本判决系数方法，数据样本量不能太小，否则无法支持数据切分。第二，结果有可能不稳定，因为数据的切分是随机的，所产生的结果也是随机的。如果数据样本量足够大，该随机性有可能很小可忽略。但是，如果数据样本量不够大，那么该不稳定性就明显。一个可能的解决方法就是重复多次试验，最后获得一个稳定的综合判断。 2.5.6 多重共线性 只要有一个解释变量能够被其他几个很好的线性表出，线性模型就会受到多重共线性的影响。怎么度量一个解释变量受多重共线性影响的严重程度呢？可以拟合下面的线性模型： \\[X_j=β_0+\\sum_{k≠j}β_kX_k+ε\\] 简单来说，就是用第j个解释变量\\(X_j\\)作为因变量，而其它的解释变量作自变量重新拟合一个线性回归模型。产生的判决系数记做\\(R_j^2\\)。为分析方便，定义如下方差膨胀因子，即 \\[VIF_j=\\frac{1}{1-R_j^2}\\] 从理论上讲，VIF越大，相应的解释变量所承受的多重共线性影响越大。对于实际问题，很多学者遵循一个原则，就是不要大于10。 2.5.7 Cook距离 方差膨胀因子能够判断一个解释变量是否受多重共线性影响。从另一个角度看，这在帮助判断是否一个解释变量的存在会极大地伤害整个回归结果的有效性。同理，可以对每一个样本问类似的问题：现有的样本中，是否存在这样的“少数揣蛋鬼”？它们的存在极大地影响了回归分析的结果。在统计学理论中，这样的“捣蛋鬼”称为影响点。怎样判断一个样本是不是影响点呢？ 著名的统计学家库克想出了一个非常巧妙的办法。他说可以为每一个样本打分，给影响力特别大的样本打高分，给影响力不大的样本打低分，然后就可以根据分值高低判断谁是“捣蛋鬼”。该打分后来被人们称作Cook距离。具体如何打分呢？库克说那就看该样本影响力的大小。但是影响力又是什么?应该如何测量?库克认为如果一个样本有没有它所产生的回归分析结果基本一样，那么该样本就没有什么影响力；相反，如果一个样本的存在与否极大地影响着相应的回归分析结果，那么它的影响力就很大。数学上怎么测量？\\(\\hat{β}\\)是基于所有样本所获得的最小二乘估计，对于一个给定的样本i，如果要判断它的影响力，将该样本剔除之后，重新计算最小二乘估计，记为\\(\\hat{β}_{(-i)}\\)。对比\\(\\hat{β}\\)与\\(\\hat{β}_{(-i)}\\)之间的差异。定义Cook距离如下： \\[D_j=\\frac{(\\hat{β}-\\hat{β}_{(-i)})&#39;(X&#39;X)(\\hat{β}-\\hat{β}_{(-i)})}{(p+1)\\hat{σ}^2}\\] 式中，\\(X=(X_1,X_2,…,X_n)&#39;\\)是设计矩阵。如果发现，少数一两个样本的Cook距离特别大，应该考虑将此类样本剔除后，重新拟合回归分析。 2.5.8 模型选择 AIC 理论思想是假设在考虑的所有线性模型中，没有任何一个模型是“真模型”。模型选择的目标不是挑选那个唯一正确的“真模型”，而是挑对“真模型”逼近最好的线性模型。为此，它采用了Kullack-Leibler距离去测量各个线性模型同那个所谓的“真模型”之间的差异，并尝试寻找差异最小的那一个。该差异可以用下面这个统计量估计: \\[AIC=n\\{log(\\frac{RSS}{n})+1+log(2π)\\}+2(p+1)\\] 式中，RSS是残差平方和；(p+1)是选入模型的变量个数(这里包含截距项)；n为样本量。当选入模型的变量增加时，模型变得越来越复杂，p变得越来越大，因此拟合残差平方和RSS是减少的，但是惩罚项2(p+1)是增加的。如果由解释变量增加带来的方差减少很大时，AIC的值就会减小，这说明新增的变量很重要。否则，AIC的值将会增加，这说明新增的变量不重要。因此，使用AIC选择变量的原则是:使AIC达到最小的模型是“最优”模型。因为该模型有可能最好地逼近那个所谓的“真模型”，预测精度有可能最优。 BIC BIC假设在考虑的所有线性模型中确实存在一个所谓的“真模型”，而真实数据就是根据这个线性模型生成的。该模型中涉及的X指标都是重要的，即相应的β系数不为0，而其他不重要的X指标已经被剔除在外了。假设有p个X指标，每个指标都有可能被“真模型”采纳或不采纳两种可能，这就产生了\\(2^p\\)个待选模型。施瓦茨采用了贝叶斯学派的思想，给每一个模型赋予一个先验概率。然后，基于观测到的X和Y数据，再反推各个待选模型的后验概率。他惊奇地发现，只要样本量足够大，这个后验概率的大小跟先验概率关系不大，而主要由下面这个统计量决定： \\[BIC=n\\{log(\\frac{RSS}{n})+1+log(2π)\\}+log(n)(p+1)\\] BIC越小，对应模型是“真模型”的后验概率就越高。与AIC准则函数相比，当n≧8时log(n)&gt;2，此时BIC的惩罚力度比AIC大。因此BIC选出来的变量个数往往少于AIC选出的个数。 AIC认为“真模型”不可能是线性的。AIC这个性质称为损失有效性。BIC不具备该优良性质，BIC是假设“真模型”就是一个被我们考虑的线性模型。这个性质叫做“选择相合性”。 2.6 R代码实现 a = read.csv(&quot;第1章.csv&quot;,header=T,fileEncoding = &quot;GBK&quot;) names(a) = c(&quot;Y&quot;, &quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;) a[c(1:5),] ## Y X1 X2 X3 ## 1 0.2126197 2.822822 0.9037594 0.21954887 ## 2 0.2756156 2.628389 0.9717647 0.02823529 ## 3 0.1687526 2.537819 0.9913043 0.22318841 ## 4 0.1544425 3.201124 0.8986784 0.11264947 ## 5 0.3337990 3.132580 0.8467207 0.15327929 #描述统计分析，绘制直方图 par(family=&#39;STKaiti&#39;) par(mfrow = c(2,2)) hist(a$Y, xlab = &quot;间接价值&quot;, ylab = &quot;频数&quot;,main=NULL) hist(a$X1, xlab = &quot;通话时长（百分钟）&quot;, ylab = &quot;频数&quot;,main=NULL) hist(a$X2, xlab = &quot;大网占比（%）&quot;, ylab = &quot;频数&quot;,main=NULL) hist(a$X3, xlab = &quot;小网占比（%）&quot;, ylab = &quot;频数&quot;,main=NULL) #对各个指标计算描述统计量 N = sapply(a,length) #有效样本量 sapply：输 入为列表，返回值为向量 MU = sapply(a,mean) #样本均值 SD = sapply(a,sd) #样本标准差 MIN = sapply(a,min) MED = sapply(a,median)#中位数 MAX = sapply(a,max) result = cbind(N,MU,SD,MIN,MED,MAX) #cbind:根据列进行合并;rbind:根据行进行合并 result ## N MU SD MIN MED MAX ## Y 1123 0.1930176 0.1324444 -0.49801016 0.1870535 0.9925818 ## X1 1123 2.5804381 0.4083604 0.77815125 2.5831988 3.6009729 ## X2 1123 0.8457207 0.1538765 0.09867452 0.8983452 1.0000000 ## X3 1123 0.2521307 0.2112832 0.00000000 0.1983730 0.9734904 #对每个X变量结合Y做进一步地描述统计分析 a$cat = as.factor((a$Y &gt; median(a$Y))*1) #以Y的中位数为阈值，将数据分为两组 levels(a$cat) = c(&quot;低&quot;,&quot;高&quot;) a$cat = factor(a$cat,levels=c(&quot;高&quot;,&quot;低&quot;)) par(family=&#39;STKaiti&#39;) par(mfrow = c(1,3)) boxplot(X1~cat,data=a,xlab=&quot;间接价值&quot;, ylab=&quot;通话时长（百分钟）&quot;) boxplot(X2~cat,data=a,xlab=&quot;间接价值&quot;, ylab=&quot;大网占比（%）&quot;) boxplot(X3~cat,data=a,xlab=&quot;间接价值&quot;, ylab=&quot;小网占比（%）&quot;) #建立模型 fit = lm(Y~X1+X2+X3, data = a) summary(fit) ## ## Call: ## lm(formula = Y ~ X1 + X2 + X3, data = a) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.62789 -0.04540 -0.01281 0.03177 0.62598 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.488566 0.026160 -18.676 &lt; 2e-16 *** ## X1 0.235569 0.006519 36.135 &lt; 2e-16 *** ## X2 0.089486 0.022915 3.905 9.98e-05 *** ## X3 -0.007807 0.016586 -0.471 0.638 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08826 on 1119 degrees of freedom ## Multiple R-squared: 0.5571, Adjusted R-squared: 0.5559 ## F-statistic: 469.1 on 3 and 1119 DF, p-value: &lt; 2.2e-16 #拟合优度 nsimu = 1000 ss = length(a[,1]) ss0 = round(ss*0.8) R2 = rep(0,nsimu) for(i in 1:nsimu){ a = a[order(runif(ss)),] a0 = a[c(1:ss0),] a1 = a[-c(1:ss0),] fit0 = lm(Y~X1+X2+X3, data=a0) Y.hat = predict(fit0, a1) Y.true = a1$Y sse = sum((Y.hat-Y.true)^2) sst = sum((Y.true-mean(Y.true))^2) R2[i] = (1-sse/sst)*100 } par(mfrow = c(1,1)) boxplot(R2) #VIF计算 library(car) ## Loading required package: carData vif(fit) ## X1 X2 X3 ## 1.020728 1.790661 1.768610 plot(fit,which = 4) #Cook距离 plot(fit, which = 1) #残差图 #模型选择 model.aic = step(fit, trace=F) model.bic = step(fit, k=log(ss), trace=F) summary(model.bic) ## ## Call: ## lm(formula = Y ~ X1 + X2, data = a) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.63101 -0.04534 -0.01273 0.03086 0.62575 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.495837 0.021105 -23.493 &lt; 2e-16 *** ## X1 0.235294 0.006491 36.252 &lt; 2e-16 *** ## X2 0.096597 0.017225 5.608 2.58e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08823 on 1120 degrees of freedom ## Multiple R-squared: 0.557, Adjusted R-squared: 0.5562 ## F-statistic: 704.1 on 2 and 1120 DF, p-value: &lt; 2.2e-16 library(glmnet) ## Loading required package: Matrix ## Loaded glmnet 4.1-3 attach(a) cv.outlas&lt;-cv.glmnet(cbind(X1,X2,X3),Y,alpha=1) plot(cv.outlas) ##cross-validation curve (bestlam&lt;-cv.outlas$lambda.min) ## [1] 0.0004862177 predict(cv.outlas,type=&#39;coefficients&#39;,s=bestlam) ## 4 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) -0.484951321 ## X1 0.234435944 ## X2 0.088173782 ## X3 -0.006143331 plot(cv.outlas$glmnet.fit,xvar=&quot;lambda&quot;) #coefficient plot fit.lasso &lt;- glmnet(cbind(X1,X2,X3),Y,alpha=1,lambda = bestlam) "],["逻辑回归以上市企业特别处理st为例.html", "第 3 章 逻辑回归——以上市企业特别处理ST为例 3.1 0-1型Y 3.2 案例介绍 3.3 数据介绍 3.4 描述分析 3.5 模型估计 3.6 预测评估 3.7 R编程", " 第 3 章 逻辑回归——以上市企业特别处理ST为例 3.1 0-1型Y 0-1型Y在营销实践中无处不在。例如，消费者面对一个产品，买还是不买，这就是一个0-1型数据。营销学者尝试去理解这个行为有哪些X可以解释。广告这个X可以解释吗?促销这个X可以解释吗?价格这个X可以解释吗?还是渠道这个X?或者其他?当一个消费者在互联网上漫无目的地搜索，突然跳出来一个广告，请问他点还是不点?这又是一个重要的0-1型因变量。假设，你是一个电商应用的产品经理，你关心个性化推荐算法。你需要理解，对一个什么样的人(人口统计学X)，如何根据他的购买习惯(购买记录相关的X)，选择合适的场景(时间地点相关的X)，推荐最合适的产品(产品品类相关的X)，会产生什么样的购买行为(购买与否，0-1型Y)。 0-1型Y在运营中无处不在，以移动通信运营商为例，过去的运营商工作主要以发卡为主。但是，现在手机已经非常普及，很多人甚至有多个手机号码，指望通过增加发卡量来刺激业务增长已经不再现实。因此，各大运营商开始重视客户关系管理，一方面稳住现有客户，一方面去竞争对手那里挖墙脚。从稳住自己高价值客户的角度看，运营商关心：什么样的客户(客户历史消费相关的X)面对什么样的诱惑(竞争对手营销措施相关的X)，在什么样的挽留手段下(挽留手段相关的X)，会有怎样的去留决定(是否流失，0-1型Y)。同时，运营商还关心，竞争对手那边什么样的客户(客户特征相关的X)，通过什么样的营销手段(广告相关的X)，传递什么样的产品信息(产品相关的X)，能把他拉过来(是否转化为我们的客户，0-1型Y)。 0-1型Y在投资中无处不在，最简单的例子是买卖股票。如果你能准确预测某只股票明日股价的涨跌(涨=1，跌,=0，0-1型因变量Y)，你就会获得可观的超额收益。稍微复杂一点，假设你做天使投资，你投资的都是非常早期的项目，风险巨大，请问哪一个项目才有可能获得一个阶段性成果(例如A轮融资)?这又是一个重要的0-1型Y。很多人说，这要看人，创始人很重要。因此，你会考虑一堆跟创始人相关的X。他的年龄、教育程度、工作经历、创业经历等都是相关的X变量。有人还说，这要看行业。如果这是对的，你需要一堆关于行业的X。例如，相关的产业政策，行业的增长率，行业的利润情况，行业的竞争态势等，这都是非常重要的X，还有人说，这要看整个核心团队的技术实力，因为毕竟只有技术才是真正的壁垒。因此，你需要很多关于该团队技术实力的X指标。例如，博士数目、专利数目、相关认证等。如果一个投资商能建立这些X和Y(如是否获得A轮融资)之间的强相关关系，那么他的天使投资行为将承担更小的风险，拥有更大的胜算。 0-1型Y在信贷业务中无处不在。假设你是一个银行的主管，负责信贷业务。面对陌生申请人的时候，应该如何决策?你面临的问题就是:如果把钱借给这位陌生人，他能连本带息准时偿还吗?他违约的可能性有多大?为此，需要一个回归分析模型去理解，对什么样的申请人(申请人相关的X，如职业，教育水平，收入水平，历史信用记录等)，发放什么样的贷款产品(信贷产品相关的X，如授信额度、期限、还款形式等)，他是否会准时偿还(还是不还?0-1型Y)。 综上所述，0-1型因变量丫在实际生活和工作中几乎无处不在。因为人生就是由一系列决策构成的。从个人生活到企业决策，再到家战略，都面临一系列的决策问题。而决策就是一个0-1型的选择问题。如果想研究、理解并优化我们的选择和决策，就需要构建以0-1型丫为目标的回归分析模型。 3.2 案例介绍 特别处理(special treatment,ST)政策 ST政策是我国股市特有的一项旨在保护投资者利益的政策。具体地说，当上市公司出现财务状况或其他状况异常，导致投资者难以判断公司前景，投资者利益可能受到损害时，交易所要对该公司股票交易实行ST。被ST的股票每日涨跌幅度是受到限制的。正常情况下，证监会规定一只股票的每日最高涨跌幅为10%，而被ST的股票其日涨跌幅被限制以在5%内，这样政策性的限制约束了该股票的日内波动幅度。如果把一只股票收益率的波动幅度看作其风险的一个重要表达，限制股票的每日涨跌幅度似乎可以在一定程度上控制风险。除了涨跌幅度限制，对被ST的股票，证监会要求在原股票名称之前加上提醒性注释“ST”，同时，该上市公司的中期报告必须审计。如果一个ST企业仍然持续亏损，那么它将有被退市的风险。不过对一只被ST的股票而言，虽然其每日涨跌幅度不能超过5%，但是它可以通过连续的涨停板或者跌停板使得周度收益率变化幅度仍然极大。因此，限制日度收益率的波动幅度能否减小周度(甚至月度)收益率是一个有争议的话题。什么样的企业会被ST呢?在上海证券交易所公布的《上海证券交易所股票上市规则(2008年修订)》第十三章特别处理中有详细规定。具体摘录如下： 13.2.1 上市公司出现以下情形之一的，本所对其股票交易实行退市风险警示： (一)最近两年连续亏损(以最近两年年度报告披露的当年经审计净利润为依据）； (二)因财务会计报告存在重大会计差错或者虚假记载，公司主动改正或者被中国证监会责令改正后，对以前年度财务会计报告进行追溯调整，导致最近两年连续亏损； (三)因财务会计报告存在重大会计差错或者虚假记载，被中国证监会责令改正但未在规定期限内改正，且公司股票已停牌两个月； (四)未在法定期限内披露年度报告或者中期报告，且公司股票已停牌两个月； (五)公司可能被解散； (六)法院受理关于公司破产的案件，公司可能被依法宣告破产； (七)本所认定的其他情形。 3.3 数据介绍 大股东占款如何影响上市企业的ST风险？本章构造了一个衡量大股东占款程度的指标。该指标越大，越有理由怀疑大股东占款行为。本章关心的问题是利用第t-3年的财务指标预测第t年的ST状况，因此，因变量是该企业的ST状态，一共有两种可能的状态：ST以及非ST。因此，数学上可以用0-1变量表示。例如，可以定义Y=0表示非ST，Y=1表示ST。0-1变量的所有随机特征都由概率P(Y=1)=p决定。也可以说是由P(Y=0)=1-p决定。因为不同企业的财务状况不同，所有不同样本的概率p也不同，它是一个企业财务状况的函数，如果该企业的财务状况已经被某解释变量X充分表达，那么p应该是X的一个函数，记作p(X)，选择什么样的数学函数去描述p(X)，采用什么样的方法去估计其中的未知参数，是本章的重要内容。 3.3.1 指标设计 哪些公开的财务指和公司是否ST相关? 1.ARA(\\(X_1\\)) 该指标是应收账款与总资产的比例，它反映的是盈利质量。应收账款的特点：第一，没有流动性；第二，高风险。从企业运营的角度看，应收账款应该越少越好。 2.ASSET(\\(X_2\\)) 该指标是对数变换后的资产，用于反映公司规模。 3.ATO(\\(X_3\\)) 该指标是资产周转率，它是衡量企业资产管理效率的重要财务比率。根据定义，它是一个企业在一定时期内(如一年以内)的销售收入净额除以资产平均总额而得。资产周转率在财务分析指标体系中具有重要地位。通过该指标的对比分析，能在一定程度上了解企业总资产的运营效率。一般情况下，这个数值越高，表明企业总资产周转速度越快。销售能力越强，资产利用效率越高，经营状况越好，被ST的风险应该越低。 4.ROA(\\(X_4\\)) 该指标是资产回报率，按照定义，它是一个企业在一定时期内(如一年以内)的利润总额除以总资产，它反映的是每单位资产能够给企业带来的利润。该指标可以看作对企业盈利能力的反映。 5.GROWTH(\\(X_3\\)) 该指标是销售收入增长率。按照定义，它是一个企业在一定时期内(如一年以内)的销售总额除以前一个时期的销售总额而得，它反映的是企业的增长速度。 6.LEV(\\(X_6\\)) 该指标是债务资产比率，也叫作杠杆水平。按照定义，它是一个企业债务在其总资产中所占的比率，反映的是企业的总资产中来自债权人的比率。 7.SHARE(\\(X_7\\)) 该指标是企业第一大股东持股比率，反映的是该企业的股权结构。如果企业的第一大股东持股比例很高(如大于50%)，说明该企业一股独大，其持有者对企业的方方面面具有绝对的话语权；如果企业的第一大股东持股比例很低(如小于10%)，说明该企业股权非常分散，没有人为企业的长期利益负责。 3.4 描述分析 本章用于教学的数据样本量为684，其中有36个样本为ST样本(即ST=1)，其他是正常样本(即ST=0)，ST样本占总样本的比率为5.26%。本案例涉及7个解释变量，都是连续型的X变量，它们分别为：应收账款占比(ARA)、对数资产规模(ASSET)、资产周转率(ATO)、资产回报率(ROA)、销售收入增长率(GROWTH)、杠杆水平(LEV)和第一大股东持股比率(SHARE)。因此需要对每一个X变量做一个直方图，以检查数据是否存在异常。 接下来，将Y变量是否ST引入描述统计。是否ST是一个0-1变量，它将整个数据天然地分为两类(ST=0一类，ST=1另一类)。然后对每一个X变量(连续型数据)用箱线图做对比分析。 、 最后对所有变量(X和Y)做一个描述统计分析报表。 3.5 模型估计 首先考虑使用第1章的线性回归模型来研究此问题，即 \\[Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5+\\beta_6X_6+\\beta_7X_7+\\epsilon\\] 式中，Y是因变量是否ST。如果该模型可以接受，那么第1章讲的最小二乘估计、假设检验等方法都适用。但是，可以发现这是一个无法接受的模型，因为该模型等号的左右两边是矛盾的。等号的右边是一个取值任意的量，尤其是在随机噪声\\(\\epsilon\\)存在的情况下；左边却是一个取值0-1的变量Y。 Y是一个取值为0-1的变量。因此，在给定\\(X=(1,X_1,X_2,…,X_7)&#39;\\)的情况下，其随机规律完全由条件概率\\(P(Y|X)=p(x)=p(X)=p(X&#39;\\beta)\\)决定，其中\\(\\beta=(\\beta_0,\\beta_1,\\beta_2,…,beta_7)&#39;\\)是未知的回归系数，而\\(X&#39;\\beta=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5+\\beta_6X_6+\\beta_7X_7\\)是回归系数\\(\\beta\\)和X向量的线性组合。因此，只要能够对p(X’\\(\\beta\\))的函数形式做出一个合理的数学上不矛盾的假设，就能够获得一个基本合理的模型。p(X’\\(\\beta\\))是概率，取值01之间，而X’\\(\\beta\\)作为一个一般的线性组合取值任意。因此，需要一个单调函数:它能够把一个取值任意的线性组合X’\\(\\beta\\)，以单调变换的形式变换到实数01之间。逻辑变换是最常用的一个变换，具体函数形式是：\\(p(t)=exp(t)/\\{1+exp(t)\\}\\)。很容易验证，这是一个单调递增函数，能够把任意实数映射到(0，1)开区间上。因此，这是一个可以接受的变换。对应的模型(即逻辑回归模型，logit model)的数学形式如下: \\[p(X&#39;\\beta)=\\frac{exp(X&#39;\\beta)}{1+exp(X&#39;\\beta)}\\] 它有另一个等价的数学表达形式，如下: \\[logit\\{p(X&#39;\\beta)\\}=log\\{\\frac{p(X&#39;\\beta)}{1-p(X&#39;\\beta)}\\}\\] \\(p(X&#39;\\beta)\\)本身是一个未知参数，无法用最小二乘法。考虑极大似然准则。具体地说，采用\\((Y_i,X_i)\\)代表来自第i个个体的数据。其中\\(Y_i\\)是因变量，\\(X_i=(X_{i1},…,X_{ip})&#39;\\)是相应的解释变量。给定\\(X_i\\)后，\\(Y_i\\)取值为0或1的概率分别为： \\[ P(Y_i|X_i)= \\begin{cases} \\frac{exp(X_i&#39;\\beta)}{1+exp(X_i&#39;\\beta)},\\,\\,Y_i=1\\\\ \\frac{1}{1+exp(X_i&#39;\\beta)},\\,\\,Y_i=0\\\\ \\end{cases} \\] 把这两个表达式整合在一起，可得到: \\[p(Y_i|X_i)=\\{\\frac{exp(X_i&#39;\\beta)}{1+exp(X_i&#39;\\beta)}\\}^{Y_i}\\{\\frac{1}{1+exp(X_i&#39;\\beta)}\\}^{1-Y_i}\\] 假设独立性，它们的联合似然函数为： \\[\\prod_{i-1}^np(Y_i|X_i)=\\prod_{i-1}^n\\{\\frac{exp(X_i&#39;\\beta)}{1+exp(X_i&#39;\\beta)}\\}^{Y_i}\\{\\frac{1}{1+exp(X_i&#39;\\beta)}\\}^{1-Y_i}\\] 对它做对数变换，换得到对数似然函数为: \\[L(\\beta)=\\sum_{i=1}^nlog\\{p(Y_i|X_i)\\} =\\sum_{i=1}^n[Y_ilog\\{\\frac{exp(X_i&#39;\\beta)}{1+exp(X_i&#39;\\beta)}\\}]+(1-Y_i)log\\{\\frac{1}{1+exp(X_i&#39;\\beta)}\\}\\] 然后可以通过极大化该对数似然函数\\(L(\\beta)\\)获得极大似然估计，即\\(\\hat{\\beta}=argmax_{\\beta}L(\\beta)\\)。标准的统计学理论告诉我们，该估计量是渐进无偏、相合一致的，而且是极限正态的。 可以对每个系数的估计误差有所判断，进而计算相应的P值，并做统计学推断。 定义残差为： \\[DEV=-2L(\\hat\\beta)\\] 重新定义AIC和BIC如下： \\[AIC=DEV+2×df\\] \\[BIC=DEV+log(n)×df\\] 式中，n为样本量；df为自由度。 3.6 预测评估 \\((Y_i,X_i)(i=1,2,…,n)\\)代表历史数据，假设\\((Y_i^*,X_i^*)(i=1,2,…,m)\\)是未来数据(即验证数据集)。对于未来数据，解释变量\\(X_i^*\\)是已知的，但因变量\\(Y_i^*\\)是未知的。就本案例而言，\\(Y_i\\)是某企业当年的ST状态，\\(X_i\\)是它两年前的财务指标。那么，\\(X_i^*\\)可以是另外一个企业当年的财务指标，而\\(Y_i^*\\)是它未来的ST状态。首先通过分析历史数据建立逻辑回归模型，获得极大似然估计\\(\\hat\\beta\\)。然后，将此估计应用于未来数据\\(X_i^*\\)，对其因变量\\(Y_i^*\\)取值为1的概率估计如下: \\[P(Y_i^*=1|X_i^*)≈P(X_i^{*&#39;}\\hat\\beta)=\\frac{exp(X_i^{*&#39;}\\beta)}{1+exp(X_i^{*&#39;}\\beta)}\\] 此概率量化了该企业未来被ST的可能性。如果该可能很大，更趋向于将\\(Y_i^*\\)预测为\\(Y_i^*=1\\)；否则更应该将\\(Y_i^*\\)，预测为\\(Y_i^*=0\\)。到底多大的概率才叫大，显然需要一个國值\\(\\alpha\\)，然后定义一个预测规则如下: \\[ \\hat{Y}_i^*= \\begin{cases} 1,\\,\\,P(X_i^{*&#39;}\\hat\\beta)&gt;\\alpha\\\\ 0,\\,\\,P(X_i^{*&#39;}\\hat\\beta)&lt;\\alpha\\\\ \\end{cases} \\] 不同的评判方法，量化手段会产生不同的\\(\\alpha\\)选取方法，最常见的是错判率(mis-lasification rate，MCR)，定义如下: \\[MCR=\\frac{1}{m}\\sum_{i=1}^mI(Y_i^*≠\\hat{Y_i^*})\\] 如果目标是极小化MCR，那么最优的\\(\\alpha=50\\%\\)。MCR隐含着一个假设，即不管真实的因变量\\(Y_i^*\\)是0还是1，只要判断错误所带来的损失都是一样的。如果在整个样本中\\(Y_i^*=0\\)的样本和\\(Y_i^=1*\\)的样本分布得比较平均可比，可能是一个合理的假设。 但是在现实生活中最常见的是分布非常不均匀的样本。0样本和1样本的价值是不一样的。如果把一个ST企业(ST=1)预测成为正常企业(ST=0)，后果是买入该企业的股票，并承受由于ST带来的巨大损失。相反，如果把一个正常企业(ST=0)预测成为ST企业(圣ST=1)，后果是放弃这个投资机会，这个损失完全可以通过投资其他股票获得弥补，似乎无伤大雅。这说明，在0-1样本分布不均匀的情况下，把Y=1错误预测成Y=0损失惨重，而把Y=0错误预测成为Y=1就要可忍受得多。因此，不应该简单地优化MCR，而应该根据它们的情况加权。加权错判率是其中之一，具体如下： \\[WMCR=\\frac{1}{m}\\sum_{i=1}^m\\{\\frac{I(Y_i^*≠\\hat{Y_i^*|Y_i^*=0})}{\\pi_0}+\\frac{I(Y_i^*≠\\hat{Y_i^*|Y_i^*=1})}{\\pi_1}\\}\\] 式中，\\(\\pi_0=1-\\pi_1=P(Y_i^*=0)\\)，刻画的是总体中\\(Y_i^*=0\\)的比率。本例可以大概估计\\(\\pi_0≈94.7\\%\\)，\\(\\pi_1≈5.3\\%\\)。以WMCR为标准，重新讨论阈值。 定义两个概念： \\[TPR(True Positive Rate)=P(\\hat{Y_i^*}=1|Y_i^*=1)\\] \\[FPR(False Positive Rate)=P(\\hat{Y_i^*}=1|Y_i^*=0)\\] 称TPR为灵敏性(sensitivity)，1-FPR为特异性(specificity)。 实际工作中，如何平衡灵敏性和特异性不是一个容易的问题，这牵涉到两种错误带来的损失差别有多大。对大多数实际工作而言，两种错误所带来的损失很难量化，这时无论是MCR还是WMCR都难以成为一个完美的评价标准。此时，需要一个更加综合的判断标准，不依赖于一个具体阈值\\(\\alpha\\)的选择。于是就有了ROC曲线和AUC标准。 对于任意一个给定的阈值\\(\\alpha\\)，都会产生一组对应的灵敏性和特异性值。然后，以灵敏性为横坐标(X轴)特异性为纵轴(Y轴)，就可以在一个平面上把这个点标注出来。对于一个给定的分类方法和一个给定的验证数据集合，可以通过尝试大量不同阈值\\(\\alpha\\)的选择，产生大量的(specificity，sensitivity)坐标，并因此能够在平面上标注出大量的点。随着标注的点越来越多，它们逐渐形成了一个曲线，就是ROC曲线。这条曲线有以下重要的特征: 第一，对于大多数的实际数据而言，specificity=1常常意味着将所有的样本野蛮地预测为Y=0(否则无法保证不冤枉好人)。因此，相应的sensitivity常会为0(意味着放跑了所有的坏的人)。 第二，对于大多数的实际数据而言，sensitivity=1常常意味着将所有的样本野蛮地预测为Y=1(否则无法保证抓住所有的坏的人)。因此，相应的特异性常常会为0(意味着冤枉了所有的好人)。 假设采纳的预测方法是一个很糟糕的方法，这就意味着，任意给定一个样本无论真实的情况如何(Y=0还是Y=1)，人们都以一个确定的概率\\(\\alpha\\)预测Y=1。从FPR和TPR的角度，这意味着，抓住坏人的概率和冤枉好人的概率是一样的，都是\\(\\alpha\\)。如果将这样的点标注在ROC图上，会发现它正好落在从(1，0)出发到(0，1)的对角线上。理论上可以尝试不同的\\(\\alpha\\)，并因此标出大量不同的点，它们都落在同一个对角线上，并最终将整个对角线填满。这说明对角线也是一个ROC曲线，该曲线对应的是最糟糕的预测方法:胡蒙乱猜。因此，任何有意义的模型(如逻辑回归)所产生的ROC曲线，都应该落在对角线的左上方。也就是说，对一个0-1型数据的预测方法，预期在绝大多数情况下，TPR要比FPR(或者1-specificity)大，否则这就是一个连胡蒙乱猜都不如的预测方法，因此，一个好的预测方法所对应的ROC曲线，应该尽量向坐标系的左上方凸起，越是向这个方向凸起，说明在FPR一定的情况下，TPR越高，因此该预测方法越好。更向左上方凸起的，代表着更加准确的预测能力，是个更好的预测方法。相应地，落在该曲线下的面积曲线下面积(AUC)会更大。因此，AUC就成了一个综合评价预测模型能力的指标。之所以被称为一个综合性指标，是因为它的大小不依赖于具体阈值\\(\\alpha\\)。 3.7 R编程 a=read.csv(&quot;第3章.csv&quot;,header=T) a[c(1:5),] ## ARA ASSET ATO ROA GROWTH LEV SHARE ST ## 1 0.19230963 19.85605 0.0052 0.087709802 -0.9507273 0.4458801 26.89 0 ## 2 0.22011996 20.91086 0.0056 0.016820383 -0.9426563 0.3986864 39.62 0 ## 3 0.32529169 19.35262 0.0166 0.042468332 -0.9374404 0.3033481 26.46 0 ## 4 0.02572868 21.43893 0.0028 0.018151630 -0.8529953 0.7582502 60.16 0 ## 5 0.53359089 21.61334 0.2552 0.004146607 -0.8167039 0.7268753 54.24 1 c(dim(a),sum(a$ST),mean(a$ST))#行数(样本量)、列数(字段个数)、ST样本总数、占比 ## [1] 684.00000000 8.00000000 36.00000000 0.05263158 par(family=&quot;STHeiti&quot;) hist(a$ARA,main=&quot;应收账款占比&quot;,xlab=&quot;ARA&quot; ,ylab = &quot;频数&quot;)#对X变量应收账款占比(ARA)做直方图 par(family=&quot;STHeiti&quot;) par(mfrow=c(2,3)) hist(a$ASSET,main=&quot;对数资产规模&quot;,xlab=&quot;ASSET&quot;,ylab = &quot;频数&quot;) hist(a$ATO,main=&quot;资产周转率&quot;,xlab=&quot;ATO&quot;,ylab = &quot;频数&quot;) hist(a$ROA,main=&quot;资产回报率&quot;,xlab=&quot;ROA&quot;,ylab = &quot;频数&quot;) hist(a$GROWTH,main=&quot;销售收入增长率&quot;,xlab=&quot;GROWTH&quot;,ylab = &quot;频数&quot;) hist(a$LEV,main=&quot;杠杆水平&quot;,xlab=&quot;LEV&quot;,ylab = &quot;频数&quot;) hist(a$SHARE,main=&quot;第一大股东持股比率&quot;,xlab=&quot;SHARE&quot;,ylab = &quot;频数&quot;) par(family=&quot;STHeiti&quot;) par(mfrow=c(1,1))#par()一页多图 boxplot(ARA~ST,ylab=&quot;应收账款占比&quot;,xlab=&quot;是否ST&quot;,data=a,names=c(&quot;非ST&quot;,&quot;ST&quot;))#箱线图对ARA做对比分析 par(family=&quot;STHeiti&quot;) par(mfrow=c(2,3)) boxplot(ASSET~ST,ylab=&quot;对数资产规模&quot;,xlab=&quot;是否ST&quot;,data=a,names=c(&quot;非ST&quot;,&quot;ST&quot;)) boxplot(ATO~ST,ylab=&quot;资产周转率&quot;,xlab=&quot;是否ST&quot;,data=a,names=c(&quot;非ST&quot;,&quot;ST&quot;)) boxplot(ROA~ST,ylab=&quot;资产回报率&quot;,xlab=&quot;是否ST&quot;,data=a,names=c(&quot;非ST&quot;,&quot;ST&quot;)) boxplot(GROWTH~ST,ylab=&quot;销售收入增长率&quot;,xlab=&quot;是否ST&quot;,data=a,names=c(&quot;非ST&quot;,&quot;ST&quot;)) boxplot(LEV~ST,ylab=&quot;杠杆水平&quot;,xlab=&quot;是否ST&quot;,data=a,names=c(&quot;非ST&quot;,&quot;ST&quot;)) boxplot(SHARE~ST,ylab=&quot;第一大股东持股比率&quot;,xlab=&quot;是否ST&quot;,data=a,names=c(&quot;非ST&quot;,&quot;ST&quot;)) N=sapply(a,length) #有效样本量 MU=sapply(a,mean) #样本均值 SD=sapply(a,sd) #样本标准差 MIN=sapply(a,min) MED=sapply(a,median)#中位数 MAX=sapply(a,max) result=cbind(N,MU,SD,MIN,MED,MAX) #cbind:根据列进行合并;rbind:根据行进行合并 result ## N MU SD MIN MED MAX ## ARA 684 0.09504945 0.09228931 0.00000000 0.06832718 0.6346842 ## ASSET 684 20.77785347 0.83352322 18.66070036 20.70050279 24.0176107 ## ATO 684 0.51977383 0.36282648 0.00280000 0.43340000 3.1513000 ## ROA 684 0.05587011 0.03859391 0.00008170 0.05125798 0.3111300 ## GROWTH 684 0.11525745 0.30702005 -0.95072732 0.10228264 0.9985565 ## LEV 684 0.40606356 0.16576397 0.01843107 0.40673974 0.9803218 ## SHARE 684 46.03451754 17.68437717 4.16000000 44.95500000 88.5800000 ## ST 684 0.05263158 0.22346029 0.00000000 0.00000000 1.0000000 model.full=glm(ST~ARA+ASSET+ATO+GROWTH+LEV+ROA+SHARE,family=binomial(link=logit),data=a) #glm建立逻辑回归模型; family=binomial:Y是0-1型因变量; link=logit:逻辑回归 summary(model.full) ## ## Call: ## glm(formula = ST ~ ARA + ASSET + ATO + GROWTH + LEV + ROA + SHARE, ## family = binomial(link = logit), data = a) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4165 -0.3354 -0.2536 -0.1958 3.0778 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.86924 4.63586 -1.913 0.05573 . ## ARA 4.87974 1.49245 3.270 0.00108 ** ## ASSET 0.24660 0.22409 1.100 0.27115 ## ATO -0.50738 0.65744 -0.772 0.44026 ## GROWTH -0.83335 0.56706 -1.470 0.14167 ## LEV 2.35415 1.20138 1.960 0.05005 . ## ROA -0.63661 6.22354 -0.102 0.91853 ## SHARE -0.01111 0.01115 -0.997 0.31891 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 282.07 on 683 degrees of freedom ## Residual deviance: 251.51 on 676 degrees of freedom ## AIC: 267.51 ## ## Number of Fisher Scoring iterations: 6 (1)从中可以看到，在5%的显著性水平下，只有ARA一个变量高度显著，符号为正。这说明，一个企业的ARA取值越大，应收账款占比越高，收入质量越差，越有可能被大股东占款，因此被ST的可能性越大。如果将显著性水平稍微放宽一点，LEV也会显著，符号为正。这说明，一个企业的LEV取值越大，财务杠杆越高，负债越重，因此被ST的可能性越大。 (2)空模型(只含有截距项的逻辑回归模型)的deviance是282.07，(Null deviance )对应的自由度是：样本量(684)-1(截距项)=683。残差的deviance是251.51，相应的自由度是：样本量(684)-1(截距项)-X变量个数(7)=676。因此，可以做一个关于该模型全局显著性的卡方检验。统计量为：282.07-251.51=30.56，对应的应该是一个自由度为683-676=7的卡方分布。 1-pchisq(30.56,df=7)#对应的P值 ## [1] 7.493111e-05 c(AIC(model.full),BIC(model.full))#AIC=251.51+2*8=267.51；BIC=252.51+log(684)*8=303.73 ## [1] 267.5057 303.7293 model.aic=step(model.full,trace = F) summary(model.aic) ## ## Call: ## glm(formula = ST ~ ARA + GROWTH + LEV, family = binomial(link = logit), ## data = a) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4020 -0.3323 -0.2647 -0.2119 3.1063 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.6022 0.5646 -8.152 3.58e-16 *** ## ARA 5.1301 1.4341 3.577 0.000347 *** ## GROWTH -0.9061 0.5471 -1.656 0.097708 . ## LEV 2.5501 1.0778 2.366 0.017986 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 282.07 on 683 degrees of freedom ## Residual deviance: 254.04 on 680 degrees of freedom ## AIC: 262.04 ## ## Number of Fisher Scoring iterations: 6 AIC模型选择的变量在10%的水平下，都显著。 ss=length(a[,1]) model.bic=step(model.full,trace = F,k=log(ss)) summary(model.bic) ## ## Call: ## glm(formula = ST ~ ARA, family = binomial(link = logit), data = a) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3227 -0.3195 -0.2708 -0.2412 2.6962 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.6834 0.2722 -13.531 &lt; 2e-16 *** ## ARA 6.3316 1.3132 4.821 1.43e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 282.07 on 683 degrees of freedom ## Residual deviance: 261.71 on 682 degrees of freedom ## AIC: 265.71 ## ## Number of Fisher Scoring iterations: 6 #计算三个模型的AUC library(pROC) pred.full=predict(model.full,a) pred.aic=predict(model.aic,a) pred.bic=predict(model.bic,a) Y=a$ST roc.full=roc(Y,pred.full) roc.aic=roc(Y,pred.aic) roc.bic=roc(Y,pred.bic) c(roc.full$auc,roc.aic$auc,roc.bic$auc) ## [1] 0.7700189 0.7738340 0.6709962 par(family=&quot;STHeiti&quot;) par(mfrow=c(1,3)) plot(roc.full,main=&quot;全模型&quot;) plot(roc.aic,main=&quot;AIC模型&quot;) plot(roc.bic,main=&quot;BIC模型&quot;) #基于外样本的模型精度评估 nsimu=100 p=0.8 ss0=round(ss*p) AUC=as.data.frame(matrix(0,nsimu,3)) names(AUC)=c(&quot;全模型&quot;,&quot;AIC模型&quot;,&quot;BIC模型&quot;) for(i in 1:nsimu){ aa=a[order(runif(ss)),] A0=aa[c(1:ss0),] A1=aa[-c(1:ss0),] model.1=glm(ST~ARA+ASSET+ATO+GROWTH+LEV+ROA+SHARE,family=binomial(link=logit),data=A0) model.2=glm(ST~ARA+GROWTH+LEV,family=binomial(link=logit),data=A0) model.3=glm(ST~ARA,family=binomial(link=logit),data=A0) pred.1=predict(model.1,A1) pred.2=predict(model.2,A1) pred.3=predict(model.3,A1) Y=A1$ST auc.1=roc(Y,pred.1)$auc auc.2=roc(Y,pred.2)$auc auc.3=roc(Y,pred.3)$auc AUC[i,]=c(auc.1,auc.2,auc.3) } par(family=&quot;STHeiti&quot;) par(mfrow=c(1,1)) boxplot(AUC,main=&quot;外样本AUC对比&quot;) "],["定序回归信用卡逾期还款用户调查.html", "第 4 章 定序回归——信用卡逾期还款用户调查 4.1 案例数据介绍 4.2 描述统计 4.3 Modeling: Logistic regression (因变量Y1) 4.4 Modeling: 定序回归（因变量Y2,Y1=1） 4.5 预测", " 第 4 章 定序回归——信用卡逾期还款用户调查 4.1 案例数据介绍 status（逾期状态）： - 1= （1-30） - 2= （31-60） - 3= （61-90） - 4= （91-120） - 5= （121-150） - 6= （151-180） - 7= （180+） Y1: 是否按期还款 - 1= 是 - 0= 否 Y2: 逾期严重程度 - 1= （1-30） - 2= （31-60） - 3= （61-90） - 4= （91+） a=read.csv(&quot;4-mac.csv&quot;,header=T) names(a)=c(&quot;gender&quot;,&quot;usage&quot;,&quot;credit&quot;,&quot;loan&quot;,&quot;history&quot;,&quot;accounts&quot;,&quot;status&quot;) a$Y1=1*(a$status&gt;0) ##是否按期还款 a$Y2=a$status ##逾期严重程度 a[c(1:5),] ## gender usage credit loan history accounts status Y1 Y2 ## 1 女性 0.9900680 3000 0 0 0 0 0 0 ## 2 女性 1.3578084 3000 0 7 3 1 1 1 ## 3 男性 1.2766882 5000 0 7 2 3 1 3 ## 4 男性 0.7242874 3000 1745 0 0 0 0 0 ## 5 男性 1.0739191 2000 0 0 11 0 0 0 summary(a) ## gender usage credit loan ## Length:8000 Min. : 0.0000 Min. : 1000 Min. : 0.0 ## Class :character 1st Qu.: 0.2887 1st Qu.: 3000 1st Qu.: 0.0 ## Mode :character Median : 0.8379 Median : 4000 Median : 0.0 ## Mean : 0.7373 Mean : 5173 Mean : 772.9 ## 3rd Qu.: 1.1215 3rd Qu.: 5000 3rd Qu.: 0.0 ## Max. :13.0889 Max. :50000 Max. :391357.0 ## history accounts status Y1 ## Min. : 0.000 Min. : 0.000 Min. :0.000 Min. :0.0000 ## 1st Qu.: 0.000 1st Qu.: 0.000 1st Qu.:0.000 1st Qu.:0.0000 ## Median : 0.000 Median : 2.000 Median :1.000 Median :1.0000 ## Mean : 1.516 Mean : 3.365 Mean :1.445 Mean :0.6148 ## 3rd Qu.: 2.000 3rd Qu.: 4.000 3rd Qu.:2.000 3rd Qu.:1.0000 ## Max. :30.000 Max. :67.000 Max. :7.000 Max. :1.0000 ## Y2 ## Min. :0.000 ## 1st Qu.:0.000 ## Median :1.000 ## Mean :1.445 ## 3rd Qu.:2.000 ## Max. :7.000 dim(a) ## [1] 8000 9 mean(a$Y1) ## [1] 0.61475 par(family=&quot;STHeiti&quot;) par(mfrow=c(1,2)) a1=a[a$Y1==1,] a1[1:5,] ## gender usage credit loan history accounts status Y1 Y2 ## 2 女性 1.357808 3000 0 7 3 1 1 1 ## 3 男性 1.276688 5000 0 7 2 3 1 3 ## 6 女性 1.173001 3000 0 0 2 3 1 3 ## 7 男性 1.030280 5000 0 0 5 6 1 6 ## 8 女性 0.945706 4000 0 2 2 1 1 1 tab=table(a1$Y2) barplot(tab,xlab=&quot;逾期严重程度&quot;,main=&quot;合并前&quot;) a$Y2=4*(a$Y2&gt;4)+a$Y2*(a$Y2&lt;=4) a1=a[a$Y1==1,] a1[1:5,] ## gender usage credit loan history accounts status Y1 Y2 ## 2 女性 1.357808 3000 0 7 3 1 1 1 ## 3 男性 1.276688 5000 0 7 2 3 1 3 ## 6 女性 1.173001 3000 0 0 2 3 1 3 ## 7 男性 1.030280 5000 0 0 5 6 1 4 ## 8 女性 0.945706 4000 0 2 2 1 1 1 tab=table(a1$Y2) barplot(tab,xlab=&quot;逾期严重程度&quot;,main=&quot;合并后&quot;) 4.1.1 gender tab=table(a$gender) tab ## ## 女性 男性 ## 2544 5456 tab/sum(tab) ## ## 女性 男性 ## 0.318 0.682 4.1.2 usage par(family=&quot;STHeiti&quot;) hist(a$usage,xlab=&quot;信用卡相对使用率&quot;, ylab = &quot;频数&quot;,main=NULL) a[a$usage&gt;4,] ## gender usage credit loan history accounts status Y1 Y2 ## 903 男性 6.074610 50000 17268 8 4 3 1 3 ## 1333 男性 4.241007 3000 25571 2 4 4 1 4 ## 2347 男性 4.683206 10000 6811 9 1 1 1 1 ## 2357 女性 6.024418 12000 13799 2 18 1 1 1 ## 3135 男性 5.095060 1000 1774 0 4 0 0 0 ## 3722 男性 4.293418 1000 0 3 12 2 1 2 ## 3805 男性 5.271930 1000 0 0 4 4 1 4 ## 6148 女性 13.088921 10000 6550 0 3 3 1 3 ## 7069 男性 4.597112 1000 0 0 4 0 0 0 a$usage1=1*(a$usage&lt;=1) tab=table(a$usage1) tab ## ## 0 1 ## 3329 4671 tab/sum(tab) ## ## 0 1 ## 0.416125 0.583875 4.1.3 credit par(family=&quot;STHeiti&quot;) par(mfrow=c(1,2)) hist(a$credit,xlab=&quot;授信额度（元）&quot;, ylab = &quot;频数&quot;,main=NULL) a$credit=log(a$credit) hist(a$credit,xlab=&quot;对数授信额度&quot;, ylab = &quot;频数&quot;,main=NULL) 4.1.4 loan a$Z1=1*(a$loan&gt;0) a$Z2=a$loan a[a$loan&gt;0,]$Z2=log(a[a$loan&gt;0,]$Z2) a[1:5,] ## gender usage credit loan history accounts status Y1 Y2 usage1 Z1 ## 1 女性 0.9900680 8.006368 0 0 0 0 0 0 1 0 ## 2 女性 1.3578084 8.006368 0 7 3 1 1 1 0 0 ## 3 男性 1.2766882 8.517193 0 7 2 3 1 3 0 0 ## 4 男性 0.7242874 8.006368 1745 0 0 0 0 0 1 1 ## 5 男性 1.0739191 7.600902 0 0 11 0 0 0 0 0 ## Z2 ## 1 0.00000 ## 2 0.00000 ## 3 0.00000 ## 4 7.46451 ## 5 0.00000 mean(a$Z1) ## [1] 0.169625 a2=a[a$Z1==1,] par(family=&quot;STHeiti&quot;) hist(a2$Z2,xlab=&quot;对数房贷月供&quot;, ylab = &quot;频数&quot;,main=NULL) 4.1.5 history tab=table(a$history) par(family=&quot;STHeiti&quot;) barplot(tab,xlab=&quot;历史逾期次数&quot;, ylab = &quot;频数&quot;,main=NULL) 4.1.6 accounts tab=table(a$accounts) par(family=&quot;STHeiti&quot;) barplot(tab,xlab=&quot;信用卡开户数&quot;, ylab = &quot;频数&quot;,main=NULL) 4.2 描述统计 mu1=tapply(a$Y1,a$gender,mean)#性别 mu2=tapply(a$Y1,a$usage1,mean)#用信用卡 mu3=tapply(a$Y1,a$Z1,mean)#是否房贷 mu1 ## 女性 男性 ## 0.5534591 0.6433284 mu2 ## 0 1 ## 0.7179333 0.5412117 mu3 ## 0 1 ## 0.6394701 0.4937362 par(family=&quot;STHeiti&quot;) par(mfrow=c(1,3)) boxplot(credit~Y1,data=a,ylab=&quot;对数授信额度&quot;,xlab=&quot;是否逾期&quot;,names=c(&quot;非逾期&quot;,&quot;逾期&quot;)) boxplot(history~Y1,data=a,ylab=&quot;历史逾期次数&quot;,xlab=&quot;是否逾期&quot;,names=c(&quot;非逾期&quot;,&quot;逾期&quot;)) boxplot(accounts~Y1,data=a,ylab=&quot;信用卡开户数&quot;,xlab=&quot;是否逾期&quot;,names=c(&quot;非逾期&quot;,&quot;逾期&quot;)) par(family=&quot;STHeiti&quot;) par(mfrow=c(1,1)) a1=a[a$loan&gt;0,] boxplot(Z2~Y1,data=a1,ylab=&quot;对数月供&quot;,xlab=&quot;是否逾期&quot;,names=c(&quot;非逾期&quot;,&quot;逾期&quot;)) 4.2.1 有逾期的 aa=a[a$Y1==1,] aa$sex=1*(aa$gender==&quot;男性&quot;) par(family=&quot;STHeiti&quot;) par(mfrow=c(1,3)) plot(tapply(aa$sex,aa$Y2,mean),type=&quot;b&quot;,ylab=&quot;男性占比&quot;,xlab=&quot;逾期严重程度&quot;,ylim=c(0,1)) plot(tapply(aa$usage,aa$Y2,mean),type=&quot;b&quot;,ylab=&quot;正常使用率占比&quot;,xlab=&quot;逾期严重程度&quot;,ylim=c(0,1)) plot(tapply(aa$Z1,aa$Y2,mean),type=&quot;b&quot;,ylab=&quot;房贷占比&quot;,xlab=&quot;逾期严重程度&quot;,ylim=c(0,1)) par(family=&quot;STHeiti&quot;) par(mfrow=c(2,2)) boxplot(credit~Y2,data=aa,ylab=&quot;对数授信额度&quot;,xlab=&quot;逾期严重程度&quot;) boxplot(history~Y2,data=aa,ylab=&quot;历史逾期次数&quot;,xlab=&quot;逾期严重程度&quot;) boxplot(accounts~Y2,data=aa,ylab=&quot;信用卡开户数&quot;,xlab=&quot;逾期严重程度&quot;) aa1=aa[aa$loan&gt;0,] boxplot(Z2~Y2,data=aa1,ylab=&quot;对数月供&quot;,xlab=&quot;逾期严重程度&quot;) 4.3 Modeling: Logistic regression (因变量Y1) model.full=glm(Y1~gender+usage+credit+Z1+Z2+history+accounts, family=binomial(link=logit),data=a) summary(model.full) ## ## Call: ## glm(formula = Y1 ~ gender + usage + credit + Z1 + Z2 + history + ## accounts, family = binomial(link = logit), data = a) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.0363 -1.0495 0.4059 0.9847 2.0931 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.530145 0.390839 6.474 9.57e-11 *** ## gender男性 0.259764 0.054136 4.798 1.60e-06 *** ## usage 0.390785 0.056410 6.928 4.28e-12 *** ## credit -0.366917 0.045951 -7.985 1.41e-15 *** ## Z1 0.499839 0.529408 0.944 0.3451 ## Z2 -0.115344 0.067276 -1.714 0.0864 . ## history 0.594571 0.023291 25.528 &lt; 2e-16 *** ## accounts 0.008826 0.006232 1.416 0.1567 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10665.2 on 7999 degrees of freedom ## Residual deviance: 8914.6 on 7992 degrees of freedom ## AIC: 8930.6 ## ## Number of Fisher Scoring iterations: 5 变量选择 \\[A I C=\\text { Deviance }+2 \\times d f\\] \\[B I C=\\text { Deviance }+\\log (n) \\times d f \\] 1-pchisq(10665.2-8924.5,df=7) #检验模型显著性 ## [1] 0 model.aic=step(model.full,trace=F) #stepwise by AIC summary(model.aic) ## ## Call: ## glm(formula = Y1 ~ gender + usage + credit + Z2 + history, family = binomial(link = logit), ## data = a) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.0219 -1.0464 0.4037 0.9886 2.0488 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.637063 0.385156 6.847 7.56e-12 *** ## gender男性 0.262164 0.054112 4.845 1.27e-06 *** ## usage 0.415324 0.053508 7.762 8.36e-15 *** ## credit -0.378367 0.045380 -8.338 &lt; 2e-16 *** ## Z2 -0.050523 0.008836 -5.718 1.08e-08 *** ## history 0.591816 0.023187 25.523 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10665.2 on 7999 degrees of freedom ## Residual deviance: 8917.4 on 7994 degrees of freedom ## AIC: 8929.4 ## ## Number of Fisher Scoring iterations: 5 1-pchisq(10665.2-8925.1,df=6) ## [1] 0 ss=length(a[,1]) model.bic=step(model.full,trace=F,k=log(ss)) #stepwise by BIC summary(model.bic) ## ## Call: ## glm(formula = Y1 ~ gender + usage + credit + Z2 + history, family = binomial(link = logit), ## data = a) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.0219 -1.0464 0.4037 0.9886 2.0488 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.637063 0.385156 6.847 7.56e-12 *** ## gender男性 0.262164 0.054112 4.845 1.27e-06 *** ## usage 0.415324 0.053508 7.762 8.36e-15 *** ## credit -0.378367 0.045380 -8.338 &lt; 2e-16 *** ## Z2 -0.050523 0.008836 -5.718 1.08e-08 *** ## history 0.591816 0.023187 25.523 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10665.2 on 7999 degrees of freedom ## Residual deviance: 8917.4 on 7994 degrees of freedom ## AIC: 8929.4 ## ## Number of Fisher Scoring iterations: 5 1-pchisq(10665.2-8927.9,df=5) ## [1] 0 4.3.1 AUC对比 library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var pred.full=predict(model.full,data=a) pred.aic=predict(model.aic,data=a) pred.bic=predict(model.bic,data=a) roc.full=roc(a$Y1,pred.full) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases roc.aic=roc(a$Y1,pred.aic) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases roc.bic=roc(a$Y1,pred.bic) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases print(c(roc.full$auc,roc.aic$auc,roc.bic$auc)) ## [1] 0.7672396 0.7667043 0.7667043 par(family=&quot;STHeiti&quot;) par(mfrow=c(1,3)) plot(roc.full,main=&quot;全模型&quot;) plot(roc.aic,main=&quot;AIC模型&quot;) plot(roc.bic,main=&quot;BIC模型&quot;) 4.4 Modeling: 定序回归（因变量Y2,Y1=1） 4.4.1 潜变量模型 \\[\\mathrm{Z}=\\mathrm{X}^{\\prime} \\beta+\\varepsilon\\] 处理有序分类变量\\(Y_2\\), 希望通过潜变量将自变量与因变量建立联系： \\[\\mathrm{Y_2}=\\left\\{\\begin{array}{lll} 1 &amp; \\text { 如果 } &amp; \\mathrm{Z}&lt;\\mathrm{c}_{1} \\text { ； } \\\\ 2 &amp; \\text { 如果 } &amp; \\mathrm{c}_{1} \\leq \\mathrm{Z}&lt;\\mathrm{c}_{2} \\\\ 3 &amp; \\text { 如果 } &amp; \\mathrm{c}_{2} \\leq \\mathrm{Z}&lt;\\mathrm{c}_{3} \\\\ 4 &amp; \\text { 如果 } &amp; \\mathrm{c}_{3} \\leq \\mathrm{Z} \\text { ； } \\end{array}\\right. \\] 但这样的划分往往较难实现，参考logistic回归和probit回归思想，对概率\\(P(Y_2=k)\\)建模： \\[\\mathrm{P}(\\mathrm{Y_2}=\\mathrm{k})=\\left\\{\\begin{array}{ll} \\Phi\\left(\\mathrm{c}_{1}-\\mathrm{X}^{\\prime} \\beta\\right) &amp; \\text { 如果 } \\mathrm{k}=1 \\text { ； } \\\\ \\Phi\\left(\\mathrm{c}_{2}-\\mathrm{X}^{\\prime} \\beta\\right)-\\Phi\\left(\\mathrm{c}_{1}-\\mathrm{X}^{\\prime} \\beta\\right) &amp; \\text { 如果 } \\mathrm{k}=2 \\text { ； } \\\\ \\Phi\\left(\\mathrm{c}_{3}-\\mathrm{X}^{\\prime} \\beta\\right)-\\Phi\\left(\\mathrm{c}_{2}-\\mathrm{X}^{\\prime} \\beta\\right) &amp; \\text { 如果 } \\mathrm{k}=3 \\text { ； } \\\\ 1-\\Phi\\left(\\mathrm{c}_{3}-\\mathrm{X}^{\\prime} \\beta\\right) &amp; \\text { 如果 } \\mathrm{k}=4 \\text { 。 } \\end{array}\\right. \\] 需要处理此参数估计问题。常用的方法有极大似然估计。似然函数： \\[\\prod_{\\mathrm{i}=1}^{n} \\mathrm{P}\\left(\\mathrm{Y}_{\\mathrm{i}} \\mid \\mathrm{X}_{\\mathrm{i}}\\right)=\\prod_{\\mathrm{i}=1}^{\\mathrm{n}} \\prod_{\\mathrm{k}=1}^{3}\\left\\{\\mathrm{q}_{\\mathrm{k}}\\left(\\mathrm{X}_{\\mathrm{i}}^{\\prime} \\beta, \\mathrm{c}\\right)\\right\\}^{\\mathrm{I}\\left(\\mathrm{Y}_{\\mathrm{i}}=\\mathrm{k}\\right)},\\] 对数似然函数： \\[\\mathcal{L}(\\beta)=\\sum_{\\mathrm{i}=1}^{\\mathrm{n}} \\log \\left\\{\\mathrm{P}\\left(\\mathrm{Y}_{\\mathrm{i}} \\mid \\mathrm{X}_{\\mathrm{i}}\\right)\\right\\}=\\prod_{\\mathrm{i}=1}^{\\mathrm{n}} \\prod_{\\mathrm{k}=1}^{3} \\mathrm{I}\\left(\\mathrm{Y}_{\\mathrm{i}}=\\mathrm{k}\\right) \\log \\left\\{\\mathrm{q}_{\\mathrm{k}}\\left(\\mathrm{X}_{\\mathrm{i}}^{\\prime} \\beta, \\mathrm{c}\\right)\\right\\} .\\] 数学推导此处略。 library(MASS) probit.full=polr(as.factor(Y2)~gender+usage+credit+Z1+Z2+history+accounts,method=&quot;probit&quot;,Hess=T,data=aa) summary(probit.full) ## Call: ## polr(formula = as.factor(Y2) ~ gender + usage + credit + Z1 + ## Z2 + history + accounts, data = aa, Hess = T, method = &quot;probit&quot;) ## ## Coefficients: ## Value Std. Error t value ## gender男性 0.067676 0.034031 1.98867 ## usage 0.001008 0.031370 0.03215 ## credit -0.105923 0.029567 -3.58249 ## Z1 0.447592 0.366074 1.22268 ## Z2 -0.067041 0.046899 -1.42947 ## history 0.054991 0.005488 10.02097 ## accounts 0.021823 0.003507 6.22336 ## ## Intercepts: ## Value Std. Error t value ## 1|2 -1.3560 0.2492 -5.4403 ## 2|3 -0.2384 0.2488 -0.9585 ## 3|4 0.5552 0.2492 2.2285 ## ## Residual Deviance: 12612.62 ## AIC: 12632.62 tab = as.data.frame(coefficients(summary(probit.full))) tab$p.value = round(2*(1-pnorm(abs(tab$`t value`))),3) tab ## Value Std. Error t value p.value ## gender男性 0.067675543 0.034030575 1.98866881 0.047 ## usage 0.001008471 0.031370456 0.03214714 0.974 ## credit -0.105922643 0.029566760 -3.58249076 0.000 ## Z1 0.447591651 0.366074441 1.22267933 0.221 ## Z2 -0.067040877 0.046899044 -1.42947214 0.153 ## history 0.054991103 0.005487604 10.02096774 0.000 ## accounts 0.021822732 0.003506581 6.22336419 0.000 ## 1|2 -1.355979677 0.249246544 -5.44031486 0.000 ## 2|3 -0.238444897 0.248763005 -0.95852234 0.338 ## 3|4 0.555234401 0.249155346 2.22846674 0.026 probit.null = polr(as.factor(Y2)~1,method=&quot;probit&quot;,Hess=T,data=aa) summary(probit.null) ## Call: ## polr(formula = as.factor(Y2) ~ 1, data = aa, Hess = T, method = &quot;probit&quot;) ## ## No coefficients ## ## Intercepts: ## Value Std. Error t value ## 1|2 -0.7007 0.0196 -35.8202 ## 2|3 0.3910 0.0184 21.2785 ## 3|4 1.1641 0.0230 50.5037 ## ## Residual Deviance: 12802.75 ## AIC: 12808.75 1-pchisq(12802.75 - 12612.04,df=7) ## [1] 0 4.4.2 stepwise by AIC probit.aic=step(probit.full,trace=F) summary(probit.aic) ## Call: ## polr(formula = as.factor(Y2) ~ gender + credit + Z2 + history + ## accounts, data = aa, Hess = T, method = &quot;probit&quot;) ## ## Coefficients: ## Value Std. Error t value ## gender男性 0.06797 0.034029 1.997 ## credit -0.11018 0.029279 -3.763 ## Z2 -0.01018 0.005985 -1.701 ## history 0.05495 0.005364 10.244 ## accounts 0.02171 0.003374 6.436 ## ## Intercepts: ## Value Std. Error t value ## 1|2 -1.3927 0.2445 -5.6952 ## 2|3 -0.2755 0.2440 -1.1288 ## 3|4 0.5181 0.2444 2.1201 ## ## Residual Deviance: 12614.12 ## AIC: 12630.12 tab = as.data.frame(coefficients(summary(probit.aic))) tab$p.value = round(2*(1-pnorm(abs(tab$`t value`))),3) tab ## Value Std. Error t value p.value ## gender男性 0.06797161 0.034028748 1.997476 0.046 ## credit -0.11018441 0.029278501 -3.763321 0.000 ## Z2 -0.01017686 0.005984539 -1.700526 0.089 ## history 0.05494709 0.005363807 10.244047 0.000 ## accounts 0.02171411 0.003373886 6.435935 0.000 ## 1|2 -1.39272777 0.244544133 -5.695200 0.000 ## 2|3 -0.27545588 0.244027704 -1.128789 0.259 ## 3|4 0.51807676 0.244364187 2.120101 0.034 1-pchisq(12802.75 - 12614.12,df=5) ## [1] 0 4.4.3 stepwise by BIC ss2=length(aa[,1]) probit.bic=step(probit.full,trace=F,k=log(ss2)) summary(probit.bic) ## Call: ## polr(formula = as.factor(Y2) ~ credit + history + accounts, data = aa, ## Hess = T, method = &quot;probit&quot;) ## ## Coefficients: ## Value Std. Error t value ## credit -0.12271 0.028166 -4.357 ## history 0.05542 0.005358 10.345 ## accounts 0.02112 0.003334 6.334 ## ## Intercepts: ## Value Std. Error t value ## 1|2 -1.5344 0.2361 -6.5001 ## 2|3 -0.4181 0.2354 -1.7760 ## 3|4 0.3752 0.2358 1.5914 ## ## Residual Deviance: 12621.05 ## AIC: 12633.05 tab = as.data.frame(coefficients(summary(probit.bic))) tab$p.value = round(2*(1-pnorm(abs(tab$`t value`))),3) tab ## Value Std. Error t value p.value ## credit -0.12270807 0.028166195 -4.356572 0.000 ## history 0.05542479 0.005357707 10.344871 0.000 ## accounts 0.02111930 0.003334170 6.334198 0.000 ## 1|2 -1.53438168 0.236054863 -6.500106 0.000 ## 2|3 -0.41812594 0.235431197 -1.776001 0.076 ## 3|4 0.37517650 0.235753519 1.591393 0.112 1-pchisq(12802.75 - 12621.05,df=3) ## [1] 0 4.5 预测 4.5.1 0-1模型预测 new = data.frame(gender=&quot;男性&quot;,usage=1,credit=log(20000),loan=4000, history=4,accounts=3,Z1=1,Z2=log(4000)) pred=predict(model.bic,newdata=new) exp(pred)/(1+exp(pred)) ## 1 ## 0.8198925 dat = data.frame(prediction = exp(predict(model.bic,a))/(1+exp(predict(model.bic,a)))) summary(dat) ## prediction ## Min. :0.1226 ## 1st Qu.:0.4486 ## Median :0.5736 ## Mean :0.6148 ## 3rd Qu.:0.7952 ## Max. :1.0000 4.5.2 定序回归模型预测 predict(probit.bic,newdata=new) ## [1] 2 ## Levels: 1 2 3 4 "],["生存分析员工离职前工作时长的研究.html", "第 5 章 生存分析——员工离职前工作时长的研究 5.1 背景介绍 5.2 数据介绍 5.3 描述分析 5.4 原始数据 5.5 生存（KM）曲线 5.6 Modeling: 加速失效模型 5.7 Cox等比例风险模型 5.8 模型应用", " 第 5 章 生存分析——员工离职前工作时长的研究 5.1 背景介绍 前面有几章反复提到, 顾客是企业重要的无形资产。员工又何尝不是 呢? 甚至更加重要。曾经有人问一位国际知名的企业总裁: 您是怎样让顾客 满意的? 他的回答大意如下: 我的工作是让我的员工满意, 为此我为他们创 造最好的工作条件、最优厚的福利待遇。只要他们开心了, 他们会想办法让 我的顾客开心。由此可见员工在企业成长发展过程中的巨大作用。 一方面, 现代商业社会竞争激烈, 企业的核心员工往往掌握着重要的客 户资源、技术秘密, 或者至少也是熟悉该企业的文化氛围的。一旦离职, 会 对企业造成一定的损失, 甚至是致命伤。而另一方面, 通过更好的待遇条件 获得竞争对手的核心人员, 不失为一种成长的捷径。尤其是对于成长中的中 小企业, 它们缺少人才积累, 承受着巨大的市场竞争压力, 没有时间等待。 因此, “挖” 竞争对手的 “墙脚” 大概是不可避免的方式。当然, 这些企业 自己也面临被别人 “挖墙脚” 的压力。如果一个重要员工离职, 而企业需要 找到一个合适的替代, 有哪些成本呢? 首当其冲的是时间。时间不仅仅是金 钱, 更重要的是意味着商业机遇, 失之不再来。其次, 是货币成本。如果通 过一个猎头公司获得一个重要员工, 企业一般要给猎头公司支付等同于该员 工三个月薪酬的费用。这可不是一笔可以忽略的费用! 那么企业应该靠什么留住员工呢? 作者曾经就这一问题请教过很多企业 高管, 得到过很多不同的答案。其中, 有一个答案被很多人重复提到, 那就 是: 事业留人、待遇留人, 还有感情留人。对员工来讲, 企业的前景是极其 重要的。一个发展缓慢的企业不可能为员工创造更多上升提高的机会, 因 此, 无法提供一个美丽的职业蓝图。当然, 除了有一份美好的事业前景以 外, 优厚的待遇也是很重要的。美好的事业前景在一定程度上能够抵消员工 当前对待遇的要求, 他们将此要求寄托在了企业的美好明天上面。但明天终 归是明天, 当前的企业薪酬水平以及设计对稳定员工队伍很重要。最后一条 “感情留人” 是比较有中国特色的。用时髦的话说, 就是企业文化。它有丰 富的内涵, 包括企业领导的管理风格等, 它关注的核心就是员工在企业工作 得开心与否。 企业一方面要努力为员工创造良好的条件, 另一方面也要在招聘人才时有所甄别。为什么同样的公司、同样的前景、同样的薪酬、同样的工作, 有 的员工就工作得非常开心、信心十足, 而有的员工就垂头丧气, 天天想着跳 槽。俗话说, 一个巴掌拍不响。员工的离职有企业的原因, 也有员工自己的 原因。作为人力资源主管, 控制员工的招募过程, 尽量招到同本企业文化肦 围相适应的员工，对于稳定企业员工队伍无疑是很有邦助的。 但是, 如何才能够知道哪些员工同本企业更适应, 因此工作时间会更 长? 这需要结合工作经验, 对企业过去的离职员工数据做认真分析。对于该 问题, 我们关心的因变量是员工在企业的工作时间。如果把一个员工加人企 业看作 “出生,” 而离职看作 “死亡,” 那么他的工作时间就是其 “生存时 间”。因此, 需要用下面的生存数据的分析方法, 仔细研究该问题。 5.2 数据介绍 本案例的数据来源于国内某大型商业银行人力资源部门, 共有 1300 个 样本。每一个样本对应于该银行当年的一位销售人员。这些销售人员的主要 工作是销售信用卡, 因此对他们的学历、工作经验等要求都不高, 但是相应 的离职率也很高。如果数据表明该员工离职, 是指当年离职。具体地说, 我 们详细记录了以下信息。 5.2.1 工作年限 (Y) 这是我们关心的因变量, 它记录的是某个员工已经在企业工作了多长时 间, 以月计。值得一提的是, 这并不代表 Y 就是我们所说的生存时间（survival time)。因为对于那些已经彻底离职的员工, 该变量确实代表着他的生 存时间; 但是, 对于那些还没有离职的员工, 无法确定在末来的什么时间他 们会离职, 他们的生存时间大于或者等于已经实现了的工作时间 Y 。 5.2.2 是否离职 (C) 这是一个 0-1 变量。 C=1 表示该员工已经离职, 相反 C=0 表示该员 工仍然在职。这个变量极其重要, 是对因变量 Y 的一个重要补充。或者说, Y 和 C 一起才构成了因变量。生存数据（survival data）之所以被称为生存 数据, 主要是因为 Z 的存在。假设一个员工的真实生存时间为 Z , 那么 C=1 表示该员工已经离职, 这时 Y 就是真正的生存时间, 即 Y=Z ; 但是, 如果 C=0 , 我们知道 Y 不是真实的生存时间, 而真实的生存时间必须是 \\(Z \\geqslant Y\\) 。因此, Y 是一个被截断 (censored) 的数据。截断是生存数据的最本 质特点, 不是 “生存” 本身。例如, 如果这里没有截断, 我们精确地看到了 真实的生存时间 Z , 那么因为生存时间是一个非负的连续数字, 因此对 \\(\\log (Z)\\) 做一个普通的线性回归就非常好了, 没有必要费劲讨论下面的生存 分析方法。 前面介绍的是该数据的因变量, 下面介绍解释变量。 5.2.3 户籍 \\(\\left(X_1\\right)\\) 这是一个定性变量, 有两个可能的取值, 即异地还是本地。有的研究报 告表明, 对某些企业而言, 本地员工离职的可能性更大, 因为本地员工所掌 握的本地社会资源相对丰富, 面临更多的选择。但也有研究报告显示, 异地 员工有可能只是将现在的工作作为一个临时过渡, 他们可能对那些能够为其 解决户口的单位更感兴趣。 5.2.4 性别 \\(\\left(X_2\\right)\\) 这也是一个定性变量, 有两个可能的取值, 即男或女。我们有足够的理 由相信性别的重要性。例如, 某些工作需要比较大的体力投人, 不适合女 性, 那么女性的离职率可能就要高一些; 相反, 有的工作 (如幼教), 通常 女性参与得多一些, 男性员工的适应性就要相对差一些。对于本案例的银 行, 性别能否产生足够影响目前无从知晓。 5.2.5 年龄 \\(\\left(X_3\\right)\\) 这是一个定量变量, 记录的是员工当时的年龄。如果认为斍目的跳槽尝 试是年轻人的专利, 那么年龄大一些的员工有可能渴望稳定, 因此离职率会 偏低。但是, 也有一种不同的说法, 那就是员工的年龄越大, 经验越丰富, 越有可能成为竞争对手争夺的对象, 因此离职率会更高。 以上便是本案例所采集到的所有变量和指标, 非常简单, 但也不完全, 很多重要的公司层面的指标没有放进去, 如薪酬设计、教育培养、工作部门 等。这里我们仅仅希望起到抛砖引玉的作用, 希望大家能够做得更好。 5.3 描述分析 同前面几章一样, 首先对数据做描述分析。但不同的是, 生存数据的描 述分析不大容易, 至少对于初学者来说是这样。这主要是因为生存数据常常 被截断, 对于被截断的数据, 它到底应该算做多少不清楚。例如, 工作时间 为 12 个月的在职员工, 末来他到底能够工作多长时间谁也不清楚, 只知道 大于等于 12 个月。因此, 传统的基于完整数据的统计量都不再适用 (如均 值、方差), 我们需要一个新的描述量来解决该问题, 即生存函数。具体地 说, 我们关注真实的生存时间 Z , 然后定义其相关的生存函数为 \\(S(t)=P(Z&gt;t)\\) 。接下来的目标就是要估计 \\(S(t)\\) 。因为有很多生存数据被截断, 因 此不可能把整个生存函数估计出来。但能够估计出很大一部分, 通常这部分 足够大, 能够覆盖中位数。因此, 对平均生存时间的判断, 我们更多地谈到 中位数, 而不是均值。 具体地说, 我们用 \\(\\left(Y_{i}, C_{i}, Z_{i}\\right)\\) 代表来自第 i 个 \\((1 \\leqslant i \\leqslant n)\\) 个体的数 据, 其中 \\(Y_{i}\\) 是观测到的生存时间, \\(C_{i}\\) 是一个 0-1 变量, 表明该观测的生 存状态, \\(Z_{i}\\) 代表的是真实生存时间。因此如果 \\(C_{i}=1\\) , 有 \\(Y_{i}=Z_{i}\\) ; 相反, 如 果 \\(C_{i}=0\\) , 有 \\(Y_{i} \\leqslant Z_{i}\\) 。那么应该如何估计 \\(S(t)\\) 呢? 首先定义两个重要概念。 一个是在险者 (subjects at risk), 另外一个叫做事件 (event)。对于一个任 意给定的时间 t , 对我们的案例而言, 它指的就是在职时间已经超过 t 月的 个体。这样就排除了两种个体: 一类是真实在职时间不到 t 月的样本, 这部 分样本已经离职, 而且离职时在职时间不够 t 月。另外一类是真实在职时间 也许能够达到 t 月, 但因为人职时间太短, 还没有表现出来。例如 t=12 月, 而一个员工加人公司才 6 个月, 还没有离职, 那么我们知道他的真实生 存时间大于等于 6 个月, 也许他将在末来为公司工作 24 个月, 但目前无法 判定, 我们仅仅看到他过去的 6 个月。对这部分样本, 尚难以判定, 因此, 将它们也排除在外。为了叙述方便, 我们用数学符号 R(t) 来代表这些个体 的数目。考虑条件概率 \\(P(Z&gt;t+\\Delta t \\mid Z&gt;t)\\) 应该如何估算, 其中, \\(\\Delta t\\) 代表 的是一个足够小的量, 使得在时间段 \\((t, t+\\Delta t)\\) 内没有任何个体被截断。 在这个前提下, 该概率可以通过下面的公式估计: \\[P(Z&gt;t+\\Delta t \\mid Z&gt;t)=\\frac{P(Z&gt;t+\\Delta t)}{P(Z&gt;t)} \\approx \\frac{(t, t+\\Delta t) \\text { 时刻内的事件个数 }}{t \\text { 时刻的在险者个数 }}\\] 简单地说, \\((t, t+\\Delta t)\\) 时刻内的事件个数反映的就是在职时间介于 \\(t\\) 和 \\(t+\\Delta t\\) 之间的个体的个数。如果 \\(\\Delta t\\) 足够小, 这等价于说在 t 时刻 “死亡” 的 个体数目。对本案例而言, 就是在职时间为 t 的个体数目。为方便起见, 用 \\(\\varepsilon(t)\\) 代表该数目。 如果能够对任意一个时刻 t 估计出条件概率 \\(P(Z&gt;t+\\Delta t \\mid Z&gt;t)\\) , 这等 价于估计出了整个生存函数 \\(S(t)\\) 。对于一个实际数据, 我们看到的 “死亡” 时间是离散的 (虽然理论上是连续的), 因此对于绝大多数时刻 t 而言, \\(\\varepsilon(t)=0\\) , 进而有 \\(P(Z&gt;t+\\Delta t \\mid Z&gt;t)=1\\) 。这说明对于有限样本来说, \\(S(t)=S(t+\\Delta t)\\) 。因此, 生存函数 \\(S(t)\\) 的取值只可能在有事件发生的时间 点发生变化。把所有的这样的时间点收集起来构成集合 \\(T=\\{t : 存在一个个体 i , 使得 \\left.Y_{i}=t, C_{i}=0\\right\\}\\) 。然后, 对于一个任给的时间点 t , 估计 \\(S(t)\\) 如下: \\[S(t)=\\prod_{t_{i} \\leqslant t}^{t_{i} \\in T} P\\left(Y&gt;t_{i} \\mid Y \\geqslant t_{i}\\right) \\approx \\prod_{t_{i} \\leqslant t}^{t_{i} \\in T}\\left\\{1-\\frac{\\varepsilon\\left(t_{i}\\right)}{R\\left(t_{i}\\right)}\\right\\}=\\hat{S}(t)\\] 这就是著名的 Kaplan-Meier (KM) 估计。对于一个给定的时刻 t , 标 准的统计学理论已经告诉我们该估计量是相合一致的、斩进无偏的, 而且是 极限正态的。相应的标准误差为: \\[\\operatorname{var}[\\hat{S}(t)] \\approx \\hat{S}^{2}(t) \\sum_{t_{i} \\leqslant t}^{t_{i} \\in T} \\frac{\\varepsilon\\left(t_{i}\\right)}{R\\left(t_{i}\\right)\\left\\{R\\left(t_{i}\\right)-\\varepsilon\\left(t_{i}\\right)\\right\\}}\\] 基于此公式, 置信区间就可以构造出来。 5.4 原始数据 a=read.csv(&quot;6-mac.csv&quot;,header=T) nrow(a) ## [1] 1300 names(a)=c(&quot;hk&quot;,&quot;gender&quot;,&quot;age&quot;,&quot;Y&quot;,&quot;C&quot;) a$age2=floor(a$age/10) a[c(1:5),] ## hk gender age Y C age2 ## 1 异地 男 29 10 1 2 ## 2 本地 男 34 25 0 3 ## 3 异地 女 22 31 1 2 ## 4 本地 女 36 5 1 3 ## 5 本地 男 28 19 0 2 c(dim(a),mean(a$Y)) ## [1] 1300.00000 6.00000 10.01077 1-mean(a$C) #截断数据 ## [1] 0.2269231 5.4.1 处理：生存数据 library(survival) a$YS=Surv(a$Y,a$C) a[1:5,] ## hk gender age Y C age2 YS ## 1 异地 男 29 10 1 2 10 ## 2 本地 男 34 25 0 3 25+ ## 3 异地 女 22 31 1 2 31 ## 4 本地 女 36 5 1 3 5 ## 5 本地 男 28 19 0 2 19+ head(a$YS,100) ## [1] 10 25+ 31 5 19+ 23+ 15 3 35+ 19+ 31+ 16 15 17 12 9 17+ 11 ## [19] 4 16+ 17+ 8 5 7 20+ 2 12+ 2 6 2 3 19+ 3 3 7 36+ ## [37] 11+ 18+ 18+ 10 18+ 9 7 9 28+ 17 27 13+ 12+ 4 29 35 26+ 26 ## [55] 36+ 36+ 26 24+ 24+ 27 10 16 15 11 24+ 27+ 17 16 17 17 17+ 28+ ## [73] 35+ 30 35+ 28+ 35+ 26+ 14 20 8 12 24+ 26 19+ 9 37+ 30 33+ 35+ ## [91] 24 19 24+ 25+ 24+ 17 20+ 12 10 16 例如, 如果该数据被截断的比例很小, 那么真实的生存时间就会很接近 10.0 个月; 如果数据被截断的比例很大, 真实的生存数据就会高出 10.0 个 月很多, 高出的程度很难确定。这就解释了为什么这个 10.0 个月没有特别 大的实际意义。从上面的输出结果我们还可以看到, 总样本是 n=1300 个, 其中 295 个被截断, 占总样本的 22.69 % 。这说明, 在不长的一段时间内,绝大多数样本的真实 “死亡” 时间都被准确观测到, 即相应的雇员离职。离 职雇员占总样本的 1-22.69 %=77.31 % 。这是一个巨大的比例, 值得 关注。 除了基本的数据描述以外, SAS 还计算了生存函数的 KM 估计。从中 可以看到, 如果以中位数计, 平均的员工在职时间极短, 只有 5 个月左右。 但上文说到即使被低估, 员工的平均在职时间也有 11.3 个月, 这似乎非常 矛盾。其实不然。 5 个月是中位数, 11.3 个月说的是平均数。对于大多数数 据而言, 平均数和中位数差别不是那么大。本案例差别如此之大说明 “生存 时间” 是一个严重右偏的数据。通俗地讲, 这代表着一个员工的在职时间要 么很短, 要么超过一定阈值 (如 6 个月) 后, 就会变得很长。该分析的实践 意义在于告诉管理者新雇员的前 6 个月很重要。 上面的分析虽然很直观、很好地描述了因变量, 但是没有和解释变量结 合起来。以第一个解释变量户籍为例, 我们可以把所有样本按照是异地还是 本地分组, 然后分别计算 KM 估计, 最后做对比分析。在 SAS 中, 可以实 现如下:从上表可以看出, 本地、异地的样本分布相对均匀（541 对 759), 没有 严重的问题。各组的截断比例也可比 (\\(26.25\\%\\)对 \\(20.16 \\%\\)) 。然后, SAS 会对该因素（即户籍）对生存函数的影响能力做出判断。 值得注意的是, 该判断是在忽略其他解释变量 (即性别、年龄) 的情况下做 出的, 因此, 有一定的参考意义, 但不是最终答案。SAS 为此提供了三种 不同的检验结果, 两种非参数检验 (Log-Rank, Wilcoxon)、一种参数检验 即 \\(-2 \\log(LR)\\)。对于本案例而言, 用哪一个检验都不重要, 因为都高度显 著, 这说明本地异地员工的离职可能性确实有可能不同。作者有限的经验表 明, 似乎 Log-Rank 检验在实际中（特别是医学研究中）采用得更多一些, 该检验同有名的 Cox 等比例风险模型紧密相关。虽然前面的分析有助于我们判断本地异地员工的离职率确实不同, 但无 法告诉我们到底哪一个高。从上图汇报的 KM 生存函数估计中我们可以找 到答案。因为实践的生存函数 (即本地) 一致地落在了虚线生存函数（即异 地）的上方。这说明, 在任意的一个时间点上, 本地员工的生存概率都要高 于异地员工。所以, 他们的离职率应该偏低。类似地, 我们也可以对性别做 同样的分析, 如下图所示, 我们发现男性员工离职可能性更高。最后, 我们把所有员工按照年龄每 10 年为一个单位分成三组, 其中 g=2 表示 \\(20 \\sim 30\\), \\(g=3\\) 表示 \\(31 \\sim 40\\), \\(g=4\\) 表示 41 及以上。结果如下图 所示:从中不难发现, 年龄对员工的生存函数影响也不小。年龄越小, 越容易 离职。尤其是 \\(20 \\sim 30\\) 岁组（即 \\(g=2\\) ), 它的生存概率明显小于其他两组。 而其他两组差别没有那么大。这提示我们, 也许雇用 30 岁以上的员工, 更 有可能获得较长的在职时间。 5.5 生存（KM）曲线 km.fit=survfit(a$YS~1) summary(km.fit) ## Call: survfit(formula = a$YS ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 1300 112 0.914 0.00778 0.899 0.929 ## 2 1188 225 0.741 0.01215 0.717 0.765 ## 3 963 119 0.649 0.01324 0.624 0.676 ## 4 844 84 0.585 0.01367 0.558 0.612 ## 5 760 62 0.537 0.01383 0.510 0.565 ## 6 698 38 0.508 0.01387 0.481 0.536 ## 7 660 38 0.478 0.01385 0.452 0.506 ## 8 621 47 0.442 0.01378 0.416 0.470 ## 9 572 33 0.417 0.01368 0.391 0.444 ## 10 535 22 0.400 0.01360 0.374 0.427 ## 11 504 32 0.374 0.01345 0.349 0.402 ## 12 445 30 0.349 0.01331 0.324 0.376 ## 13 398 15 0.336 0.01323 0.311 0.363 ## 14 372 15 0.322 0.01315 0.298 0.349 ## 15 354 16 0.308 0.01305 0.283 0.334 ## 16 336 22 0.288 0.01289 0.263 0.314 ## 17 301 19 0.269 0.01273 0.246 0.296 ## 18 262 10 0.259 0.01265 0.236 0.285 ## 19 229 9 0.249 0.01260 0.225 0.275 ## 20 205 9 0.238 0.01256 0.215 0.264 ## 21 186 1 0.237 0.01256 0.213 0.263 ## 22 174 5 0.230 0.01256 0.207 0.256 ## 23 165 2 0.227 0.01257 0.204 0.253 ## 24 148 5 0.219 0.01260 0.196 0.246 ## 25 121 4 0.212 0.01270 0.189 0.239 ## 26 110 7 0.199 0.01287 0.175 0.226 ## 27 95 3 0.192 0.01297 0.169 0.220 ## 28 89 5 0.182 0.01311 0.158 0.209 ## 29 78 6 0.168 0.01328 0.144 0.196 ## 30 67 6 0.153 0.01343 0.128 0.181 ## 31 56 1 0.150 0.01347 0.126 0.179 ## 32 50 1 0.147 0.01353 0.123 0.176 ## 34 37 1 0.143 0.01373 0.118 0.173 ## 35 30 1 0.138 0.01408 0.113 0.169 par(family = &quot;STHeiti&quot;) #设置字体 plot(km.fit,xlab=&quot;生存时间&quot;,ylab=&quot;生存概率&quot;) plot(survfit(YS~hk,data=a),col=c(1,2),lty=c(1,2),xlab=&quot;生存时间&quot;,ylab=&quot;生存概率&quot;) legend(24,0.9,c(&quot;本地户口&quot;,&quot;异地户口&quot;),col=c(1,2),lty=c(1,2),lwd=c(1,2),cex=0.9) plot(survfit(YS~gender,data=a),col=c(1,2),lty=c(1,2),xlab=&quot;生存时间&quot;,ylab=&quot;生存概率&quot;) legend(24,0.9,c(&quot;男性&quot;,&quot;女性&quot;),col=c(1,2),lty=c(1,2),lwd=c(1,2),cex=1) plot(survfit(YS~as.factor(age2),data=a),col=c(1,2,3),lty=c(1,2,3),xlab=&quot;生存时间&quot;,ylab=&quot;生存概率&quot;) legend(24,0.9,c(&quot;20-30&quot;,&quot;30-40&quot;,&quot;40-50&quot;),col=c(1,2,3),lty=c(1,2,3),lwd=c(1,2),cex=1) 5.6 Modeling: 加速失效模型 前面的描述分析给了我们很多有益的启发。它似乎在说所有的解释变量 都对因变量有不平凡的影响作用, 但遗㨔的是描述统计无法把所有变量作为 一个整体放在一起分析。因此, 接下来将重点讨论基于生存数据的回归模型 分析。 应该如何构造一个合理的生存数据模型呢? 我们考虑一个理想情形, 那 就是真实的生存时间 Z 是观测到的。而真实的生存时间是一个非负的连续 数据, 我们可以通过一个对数变换, 把它转换到正负无穷之间。在此基础 上, 做普通线性模型如下: \\[\\log (Z)=X^{\\prime} \\beta+\\varepsilon=\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{3}+\\varepsilon\\] 同普通线性模型类似，我们也可以假设随机扰动项服从一个均值为 0 的 正态分布, 但这会使计算变得复杂。在计算机技术发达的今天, 虽然这已不 算什么问题, 但在几十年前, 还是会给人们带来很大困难的, 因此, 传统上 人们更习惯假设 \\(\\exp (\\varepsilon)\\) 服从一个指数或者 Weibull 分布。为了讨论方便, 本章假设 \\(\\exp (\\varepsilon)\\) 服从一个标准指数分布。然后定义 \\(Z_{0}=\\exp (\\varepsilon)\\) , 这代表的 是在没有解释变量影响的情况下的基准生存时间。但是, 在解释变量 X 的 影响下, 真实的生存时间便为 \\(Z_{0} \\exp \\left(X^{\\prime} \\beta\\right)\\) 。也就是说, 在原来的基准生存 时间 \\(Z_{0}\\) 上, 乘以一个加速 (如果 \\(\\exp \\left(X^{\\prime} \\beta\\right)&lt;1\\) ) 因子 \\(\\exp \\left(X^{\\prime} \\beta\\right)\\) 。因此, 人 们习惯称此模型为加速失效 (accelerated failure time, AFT) 模型。这就是 本章要介绍的生存回归模型。 对该模型, 我们应该如何估计末知参数呢? 答案仍然是极大似然估计。 更加具体地说, 如果 C=1 , 真实的生存时间是观测到的, 即 Y=Z 。那么, Y 服从的是一个参数为 \\(\\exp \\left(X^{\\prime} \\beta\\right)\\) 的指数分布。因此, 相应的似然函数为: \\(\\exp \\left(X^{\\prime} \\beta\\right) \\exp \\left\\{-Y \\times \\exp \\left(-X^{\\prime} \\beta\\right)\\right\\}\\) 。但是, 如果 C=0 , 那么真实的生存时间 是没有观测到的, 即 \\(Y \\leqslant Z\\) 。因此, 相应的生存函数为 \\(P(Z&gt;Y)= \\exp \\left\\{-Y \\times \\exp \\left(-X^{\\prime} \\beta\\right)\\right\\}\\) 。因此, 如果假设 \\(\\left(Y_{i}, X_{i}, C_{i}\\right)\\) 代表来自第 i 个样 本的数据, 相应的似然函数为: \\[\\prod_{i=1}^{n}\\left\\{\\exp \\left(X_{i}^{\\prime} \\beta\\right)\\right\\}^{\\left.I C_{i}=1\\right)} \\exp \\left\\{-Y_{i} \\times \\exp \\left(-X_{i}^{\\prime} \\beta\\right)\\right\\}\\] 对它做对数变换后, 得到对数似然函数为: \\[\\mathcal{L}(\\beta)=\\sum_{i=1}^{n} I\\left(C_{i}=1\\right) X_{i}^{\\prime} \\beta-Y_{i} \\times \\exp \\left(-X_{i}^{\\prime} \\beta\\right)\\] 然后可以通过极大化该对数似然函数获得极大似然估计, 即 \\(\\hat{\\beta}= \\operatorname{argmax}_{\\beta} \\mathcal{L}(\\beta)\\) 。标准的统计学理论告诉我们, 该估计量是渐进无偏的、相合 一致的, 而且是极限正态的。因此, 可以对每个系数的估计误差有所判断, 进而计算相应的 p -值, 再做统计学推断, 即假设检验 \\(H_{0}: \\beta_{j}=0, H_{1}: \\beta_{j} \\neq 0\\) 。 同逻辑回归以及定序回归一样, 加速失效模型没有残差这个概念, 因此无法 定义残差平方和。但可以定义离差为 \\(D E V=-2 \\mathcal{L}(\\hat{\\beta})\\) 。然后, 也可以检验 全局检验 \\(H_{0}: \\tilde{\\beta}=0, H_{1}: \\tilde{\\beta} \\neq 0\\) , 其中 \\(\\tilde{\\beta}=\\left(\\beta_{1}, \\beta_{2}, \\cdots, \\beta_{p}\\right)^{\\prime}\\) 。当某个解释变 量为多水平定性因素时, 该因素的显著性水平也可以模仿第 4 章案例计算, 过程类似, 这里不再赘述。 model.aft1=survreg(YS~hk+gender+age,data=a) summary(model.aft1) ## ## Call: ## survreg(formula = YS ~ hk + gender + age, data = a) ## Value Std. Error z p ## (Intercept) 0.86966 0.21820 3.99 6.7e-05 ## hk本地 0.30565 0.07078 4.32 1.6e-05 ## gender男 -0.27782 0.07421 -3.74 0.00018 ## age 0.05897 0.00723 8.16 3.5e-16 ## Log(scale) 0.09098 0.02539 3.58 0.00034 ## ## Scale= 1.1 ## ## Weibull distribution ## Loglik(model)= -3512.7 Loglik(intercept only)= -3566 ## Chisq= 106.57 on 3 degrees of freedom, p= 6e-23 ## Number of Newton-Raphson Iterations: 5 ## n= 1300 model.aft=survreg(YS~hk+gender+as.factor(age2),data=a) summary(model.aft) ## ## Call: ## survreg(formula = YS ~ hk + gender + as.factor(age2), data = a) ## Value Std. Error z p ## (Intercept) 2.3148 0.0738 31.37 &lt; 2e-16 ## hk本地 0.3029 0.0707 4.28 1.8e-05 ## gender男 -0.3013 0.0742 -4.06 4.8e-05 ## as.factor(age2)3 0.5839 0.0721 8.10 5.4e-16 ## as.factor(age2)4 0.6530 0.1680 3.89 0.00010 ## Log(scale) 0.0898 0.0254 3.53 0.00041 ## ## Scale= 1.09 ## ## Weibull distribution ## Loglik(model)= -3511.8 Loglik(intercept only)= -3566 ## Chisq= 108.36 on 4 degrees of freedom, p= 1.6e-22 ## Number of Newton-Raphson Iterations: 5 ## n= 1300 [*] 所有因素都是高度显著的。同异地员工相比，本地员工的在职时间更长；同女性相比，男性员工的在职时间更短；年龄越大，在职时间越长。 5.7 Cox等比例风险模型 上节探讨的加速失效模型是一种很有用的生存回归模型, 它的优点是简 单易荲, 直接研究真实生存时间, 因此非常直观。但它也有缺点, 最主要缺 点就是对生存时间的统计学分布有比较严格的要求, 最常见的分布要求是 Weibull 分布。实际数据如果不是这样, 或者说不清楚怎么办? 因此人们需 要一种有益的补充, 这就是非常有名的 Cox 等比例风险 (proportional hazard）模型, 简称 Cox 模型, 或者等比例风险模型。 从 Cox 模型的名字就可以看出来, 它直接关心的不是生存时间, 而是 一个看不见摸不着的参数一一风险 ( hazard)。数学上对风险 h(t) 的严格定 义如下: \\[h(t)=\\lim _{\\Delta t} \\frac{P(t \\leqslant Z \\leqslant t+\\Delta t \\mid Z \\geqslant t)}{\\Delta t}=\\frac{f(t)}{S(t)}\\] 其中 \\(S(t)\\) 是前面提到的生存函数, 而 \\(f(t)\\) 是生存函数的负导数, 它 也是真实生存时间 \\(Z\\) 的概率密度函数。直观上讲, 风险 \\(h(t)\\) 刻画的是一个 生存时间大于等于 \\(t\\) 的个体, 在 t 时刻立刻失效的可能性大小（当然, 该可 能性不是通过概率来刻画的)。那么风险函数 \\(h(t)\\) 和生存函数 \\(S(t)\\) 之间是 什么关系呢? 从前一个式子可以看到, 生存函数 \\(S(t)\\) 可以唯一地确定风险 函数 \\(h(t)\\) 。那么下式告诉我们, 风险函数 \\(h(t)\\) 也可以唯一地确定生存函数 \\(S(t)\\) : \\[S(t)=\\exp \\left\\{-\\int_{0}^{t} h(s) \\mathrm{d} s\\right\\}\\] 由此可见, 生存函数 \\(S(t)\\) 和风险函数 \\(h(t)\\) 之间是一种唯一确定的关 系。因此, 研究生存函数或者研究风险函数, 从数学上讲, 应该是等价的。 只是在统计模型的操作层面上讲 (如极大似然估计), 各不相同。 加速失效模型研究的是真实的生存时间, 因此从某种意义上讲, 该模型 直接研究的是生存函数同各种解释变量之间的关系。Cox 模型直接瞄准的是 风险函数。该模型认为, 不同的个体是不一样的, 它们的区别就反映在解释 变量 \\(X\\) 上。而另一方面, 这些解释变量会通过影响该个体的风险函数 \\(h(t)\\) , 进而影响其生存状况。为了表达该风险函数对解释变量 \\(X\\) 的依赖性, 我们 记它为 \\(h(t, X)\\) , 并进一步假设: \\[h(t, X)=h_{0}(t) \\exp \\left(X^{\\prime} \\beta\\right)\\] 其中 \\(h_{0}(t)\\) 是一个同解释变量 \\(X\\) 无关的基准风险函数 (baseline hazard function), 而解释变量 \\(X\\) 的作用就体现在了因子 \\(\\exp \\left(X^{\\prime} \\beta\\right)\\) 上。这就是著 名的 Cox 模型。为什么 Cox 模型又称为等比例风险模型呢? 考虑两个不同 的个体 \\(X_{1}\\) 和 \\(X_{2}\\) , 对于一个任意时刻 t , 它们承受的风险相对大小如何呢? 考虑它们的相对比如下: \\[\\frac{h\\left(t, X_{1}\\right)}{h\\left(t, X_{2}\\right)}=\\frac{h_{0}(t) \\exp \\left(X_{1}^{\\prime} \\beta\\right)}{h_{0}(t) \\exp \\left(X_{2}^{\\prime} \\beta\\right)}=\\exp \\left\\{\\left(X_{1}-X_{2}\\right)^{\\prime} \\beta\\right\\}\\] 请注意, 该比率最后的结果同时间 \\(t\\) 无关。这说明, 在 Cox 模型框架 下, 任何两个个体同时刻的风险比是和时间无关的, 它们永远等比例。这就 是 “等比例”风险模型的来源。 Cox 模型最大的妙处就是它允许基准风险 \\(h_{0}(t)\\) 的函数形式任意。也就 是说, 我们不需要为基准风险 \\(h_{0}(t)\\) 设定一种既定的函数形式, 而相应的回归系数 \\(\\beta\\) 可以照常估计。因为基准风险 \\(h_{0}(t)\\) 的函数形式可以任意, 因此 Cox 模型覆盖了一些常见的加速失效模型为其特例。但是, 这绝不是说加速 失效模型全部都是 Cox 模型的特例, 相反人们很容易找到一些加速失效模 型无法被 Cox 模型的理论框架覆盖。因此, 理论上讲, 很难说 Cox 模型或 者加速失效模型更优, 它们互为有益补充。 那么 Cox 模型到底是如何估计回归系数 \\(\\beta\\) 的呢? 方便起见, 我们定义 \\(E=\\left\\{1 \\leqslant i \\leqslant n: C_{i}=1\\right\\}\\) , 该集合收集了所有的被观测到的死亡个体。然后对 于一个任意给定的时间点 t , 定义 \\(R(t)=\\left\\{1 \\leqslant i \\leqslant n: Y_{i} \\geqslant t\\right\\}\\) 。该集合收集了 在 \\(t\\) 时刻, 还在承受风险的所有个体。那么对于任意一个死亡个体 \\(i \\in E\\) , 它的死亡时间为 \\(Y_{i}=Z_{i}\\) 。同时被观测到, 而且生存时间不小于 \\(Y_{i}\\) 的个体有 哪些呢? 都在 \\(R\\left(Y_{i}\\right)\\) 中。对比所有 \\(R\\left(Y_{i}\\right)\\) 中的个体, 为什么是 \\(i\\) 这个个体 死亡了呢? 这样一个事情在 Cox 模型的框架下, 发生的概率有多大呢? 数 学上已经证明, 该概率同 \\(R\\left(Y_{i}\\right)\\) 中的各个个体在 \\(Y_{i}\\) 时刻所承受的风险大小 有关, 呈正比例关系。因此, 可以计算此概率如下: \\[\\frac{h_{0}\\left(Y_{i}\\right) \\exp \\left(X_{i}^{\\prime} \\beta\\right)}{\\sum_{j \\in R\\left(X_{i}\\right)} h_{0}\\left(Y_{i}\\right) \\exp \\left(X_{j}^{\\prime} \\beta\\right)}=\\frac{\\exp \\left(X_{i}^{\\prime} \\beta\\right)}{\\sum_{j \\in R\\left(Y_{i}\\right)} \\exp \\left(X_{j}^{\\prime} \\beta\\right)}\\] 请注意, 该概率同基准风险 \\(h_{0}(t)\\) 无关。这就是为什么 Cox 模型不用 对此做任何设定。然后, 把所有这样的情形都考虑进来, 就获得了一个联合 函数: \\[\\prod_{i \\in E} \\frac{\\exp \\left(X_{i}^{\\prime} \\beta\\right)}{\\sum_{j \\in R\\left(Y_{i}\\right)} \\exp \\left(X_{j}^{\\prime} \\beta\\right)}\\] 这即是著名的 Partial Likelihood Function。简单地说, 就是另外一种构 思精巧的似然函数, 对其取对数后, 变为 \\[\\mathcal{L}(\\beta)=\\sum_{i \\in E} \\log \\left\\{\\frac{\\exp \\left(X_{i}^{\\prime} \\beta\\right)}{\\sum_{j \\in R\\left(Y_{i}\\right)} \\exp \\left(X_{j}^{\\prime} \\beta\\right)}\\right\\}\\] 可以通过极大化该对数似然函数获得极大似然估计, 即 \\(\\hat{\\beta}=\\operatorname{argmax}_{\\rho} \\mathcal{L}(\\beta)\\) 。 标准的统计学理论告诉我们, 该估计量是渐进无偏的、相合一致的, 而且是 极限正态的。因此, 可以对每个系数的估计误差有所判断, 进而计算相应的 p -值, 再做统计学推断, 即假设检验 \\(H_{0}: \\beta_{j}=0, H_{1}: \\beta_{j} \\neq 0\\) 。同很多其他的广义线性模型 (generalized linear model) 一样, Cox 回归没有残差这个概 念, 因此无法定义残差平方和, 但可以定义离差为 \\(D E V=-2 \\mathcal{L}(\\hat{\\beta})\\) 。然 后, 可以检验全局检验 $H_{0}: =0, H_{1}: $, 其中 \\(\\tilde{\\beta}=\\left(\\beta_{1}, \\beta_{2}, \\cdots, \\beta_{p}\\right)^{\\prime}\\)。 当某个解释变量为多水平定性因素时, 该因素的显著性水平也可以模仿第 4 章案例计算, 过程类似, 不再慗述。 model.cox=coxph(YS~hk+gender+as.factor(age2),data=a) summary(model.cox) ## Call: ## coxph(formula = YS ~ hk + gender + as.factor(age2), data = a) ## ## n= 1300, number of events= 1005 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## hk本地 -0.27325 0.76090 0.06462 -4.228 2.35e-05 *** ## gender男 0.25679 1.29277 0.06779 3.788 0.000152 *** ## as.factor(age2)3 -0.48548 0.61540 0.06605 -7.350 1.98e-13 *** ## as.factor(age2)4 -0.56030 0.57104 0.15356 -3.649 0.000263 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## hk本地 0.7609 1.3142 0.6704 0.8636 ## gender男 1.2928 0.7735 1.1319 1.4765 ## as.factor(age2)3 0.6154 1.6250 0.5407 0.7005 ## as.factor(age2)4 0.5710 1.7512 0.4226 0.7716 ## ## Concordance= 0.609 (se = 0.01 ) ## Likelihood ratio test= 93.89 on 4 df, p=&lt;2e-16 ## Wald test = 92.76 on 4 df, p=&lt;2e-16 ## Score (logrank) test = 94.16 on 4 df, p=&lt;2e-16 [*] 所有因素都是高度显著的。同异地员工相比，本地员工的风险更小，因此工作生存时间有望更长；同女性相比，男性员工的风险更大，因此工作生存时间更短；年龄越大，风险越小，因此工作时间越长。 结果同加速失效模型对比，不难发现所有参数估计的符号都是相反的。因为一个关注生存时间，而另一个关注风险函数。 5.8 模型应用 pct = 1:99/100 ptime = predict(model.aft,newdata=data.frame(hk=&quot;异地&quot;,gender=&quot;男&quot;,age2=2), type=&quot;quantile&quot;,p=pct) par(family = &quot;STHeiti&quot;) #设置字体 matplot(ptime,1-pct,xlab=&quot;在职时长&quot;,ylab=&quot;在职概率&quot;,type=&quot;l&quot;,col=&quot;darkorchid1&quot;,lwd=3) quantile(ptime) ## 0% 25% 50% 75% 100% ## 0.04885186 1.96555005 5.01556857 10.54087003 39.81353010 coefCPH = coef(model.cox) meanhk = sum(a$hk == &quot;异地&quot;)/length(a$hk) #异地户口占比 meangender = sum(a$gender == &quot;女&quot;)/length(a$gender) #女性占比 age3 = sum(a$age2 == 3)/length(a$age2) #30-40岁年龄段占比 age4 = sum(a$age2 == 4)/length(a$age2) #40-50岁年龄段占比 rMean = exp(coefCPH[1]*meanhk + coefCPH[2]*meangender+coefCPH[3]*age3+coefCPH[4]*age4) r12 = exp(coefCPH[1]*1 + coefCPH[2]*0+coefCPH[3]*0 + coefCPH[4]*0) names(r12) = names(rMean) = NULL r12/rMean ## [1] 1.037473 predict(model.cox,newdata=data.frame(hk=&quot;异地&quot;,gender=&quot;男&quot;,age2=2),type=&quot;risk&quot;) ## 1 ## 1.292774 summary(predict(model.cox,newdata=a,type=&quot;risk&quot;) ) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.4345 0.6154 0.7956 0.8738 1.0000 1.2928 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
